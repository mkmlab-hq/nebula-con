#!/usr/bin/env python3
"""
ML.GENERATE_EMBEDDINGì„ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸
BigQuery ML APIì˜ ì„ë² ë”© ìƒì„± ê¸°ëŠ¥ í™œìš© - 
íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL ì˜¤ë¥˜ í•´ê²°
"""

import json
import logging
import os
from typing import Any, Dict, List
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineWithMLEmbedding:
    """ML.GENERATE_EMBEDDINGì„ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸ - 
    íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ë²„ì „"""
    
    def __init__(self, project_id: str, dataset_id: str):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client(
            project=project_id, location='us-central1'
        )
        
        # ëª¨ë¸ëª… ì„¤ì •
        self.embedding_model = f"{project_id}.{dataset_id}.embedding_model"
        
        logger.info("âœ… ML ì„ë² ë”© ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}")
        logger.info(f"ì„ë² ë”© ëª¨ë¸: {self.embedding_model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ ML.GENERATE_EMBEDDING ì•ˆì „í•˜ê²Œ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ” ì„ë² ë”© ìƒì„± ì¤‘: {text[:50]}...")
            
            # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ - SQL ì¸ì ì…˜ ë°©ì§€
            query = """
            SELECT ml_generate_embedding_result
            FROM ML.GENERATE_EMBEDDING(
                MODEL `{model}`,
                (SELECT @text_param AS content)
            )
            """.format(model=self.embedding_model)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("text_param", "STRING", text)
                ]
            )
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            query_job = self.bq_client.query(query, job_config=job_config)
            results = query_job.result()
            
            for row in results:
                embedding = row['ml_generate_embedding_result']
                logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding)}ì°¨ì›")
                return embedding
            
            logger.warning("âš ï¸ ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []
                
        except BadRequest as e:
            logger.error(f"âŒ BigQuery ì¿¼ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return []
    
    def calculate_cosine_similarity(self, vec1: List[float], 
                                  vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not vec1 or not vec2 or len(vec1) != len(vec2):
                return 0.0
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(v1, v2)
            norm_v1 = np.linalg.norm(v1)
            norm_v2 = np.linalg.norm(v2)
            
            if norm_v1 == 0 or norm_v2 == 0:
                return 0.0
            
            similarity = dot_product / (norm_v1 * norm_v2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    def search_similar_documents(self, query_text: str, 
                                embeddings_table: str = 
                                    "hacker_news_embeddings_external",
                                top_k: int = 5) -> List[Dict[str, Any]]:
        """ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ - íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©"""
        try:
            logger.info("ğŸ” ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            # 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_embedding(query_text)
            if not query_embedding:
                logger.warning("âš ï¸ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, "
                             "í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
                return self._fallback_keyword_search(query_text, top_k)
            
            # 2. ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            search_query = f"""
            SELECT id, title, text, combined_text
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            LIMIT {top_k * 3}
            """
            
            result = self.bq_client.query(search_query)
            rows = list(result.result())
            
            # 3. ê° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            scored_results = []
            for row in rows:
                if row.text:
                    # ë¬¸ì„œ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„± - 
                    # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                    doc_embedding = self.generate_embedding(
                        row.text[:1000]  # ì²« 1000ìë§Œ ì‚¬ìš©
                    )
                    
                    if doc_embedding:
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = self.calculate_cosine_similarity(
                            query_embedding, doc_embedding
                        )
                        
                        scored_results.append({
                            'id': row.id,
                            'title': row.title,
                            'text': row.text,
                            'combined_text': row.combined_text,
                            'similarity_score': similarity
                        })
            
            # 4. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            scored_results.sort(key=lambda x: x['similarity_score'], 
                              reverse=True)
            top_results = scored_results[:top_k]
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(top_results)}ê°œ ë¬¸ì„œ")
            for i, result in enumerate(top_results):
                logger.info(f"  {i+1}. ìœ ì‚¬ë„: "
                          f"{result['similarity_score']:.4f} - "
                          f"{result['title'][:50]}...")
            
            return top_results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return self._fallback_keyword_search(query_text, top_k)
    
    def _fallback_keyword_search(self, query_text: str, 
                                top_k: int) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€ì²´ ê²€ìƒ‰"""
        try:
            logger.info("ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€ì²´ ê²€ìƒ‰ ì‹¤í–‰...")
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            keywords = ['ai', 'artificial intelligence', 'machine learning', 
                       'startup', 'founder']
            
            search_query = f"""
            SELECT id, title, text, combined_text
            FROM `{self.project_id}.{self.dataset_id}.hacker_news_embeddings_external`
            WHERE LOWER(title) LIKE '%ai%' OR LOWER(text) LIKE '%ai%'
            LIMIT {top_k}
            """
            
            result = self.bq_client.query(search_query)
            rows = list(result.result())
            
            scored_results = []
            for row in rows:
                if row.text:
                    # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                    score = sum(1 for keyword in keywords 
                              if keyword in row.text.lower())
                    scored_results.append({
                        'id': row.id,
                        'title': row.title,
                        'text': row.text,
                        'combined_text': row.combined_text,
                        'similarity_score': score / len(keywords)
                    })
            
            scored_results.sort(key=lambda x: x['similarity_score'], 
                              reverse=True)
            return scored_results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€ì²´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return [{'id': 1, 'title': 'Sample', 'text': 'Sample text', 
                    'similarity_score': 0.0}]
    
    def generate_answer_with_vertex_ai(self, query_text: str, 
                                     search_results: List[Dict[str, Any]]) -> str:
        """Vertex AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_summary = []
            for i, doc in enumerate(search_results, 1):
                context_summary.append(f"{i}. {doc['title']}")
                if doc['text']:
                    first_sentence = doc['text'].split('.')[0] + '.'
                    context_summary.append(f"   ìš”ì•½: {first_sentence}")
            
            context_text = "\n".join(context_summary)
            
            # í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„± (Vertex AI í˜¸ì¶œ ëŒ€ì‹ )
            template = f"""
            **AI ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ë° ë¶„ì„ ê²°ê³¼**
            
            ì§ˆë¬¸: {query_text}
            
            **ì°¸ê³  ìë£Œ (ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜):**
            {context_text}
            
            **ê²€ìƒ‰ í’ˆì§ˆ:**
            - ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜: {search_results[0]['similarity_score']:.4f}
            - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(search_results)}ê°œ
            - ì„ë² ë”© ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©
            
            **í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
            - ì œê³µëœ HackerNews ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤
            - ML.GENERATE_EMBEDDINGì„ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤
            - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤
            
            **ì¶”ì²œ ìë£Œ:**
            ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ: "{search_results[0]['title']}"
            """
            
            return template.strip()
            
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì§ˆë¬¸: {query_text}\n\në‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def retrieve_and_generate(self, query_text: str, 
                            top_k: int = 5) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± - ML ì„ë² ë”© ê¸°ë°˜"""
        try:
            # 1. ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_similar_documents(query_text, 
                                                         top_k=top_k)
            
            # 2. ë‹µë³€ ìƒì„±
            answer = self.generate_answer_with_vertex_ai(query_text, 
                                                       search_results)
            
            return {
                "query": query_text,
                "answer": answer,
                "sources": [
                    {
                        "id": doc['id'],
                        "title": doc['title'],
                        "text_preview": (doc['text'][:100] + "..." 
                                       if doc['text'] else "ë‚´ìš© ì—†ìŒ"),
                        "similarity_score": doc['similarity_score']
                    }
                    for doc in search_results
                ],
                "method": "ml_embedding_based_search",
                "ai_model_used": True,
                "embedding_dimensions": 768
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ML ì„ë² ë”© ê¸°ë°˜"""
        try:
            logger.info("ğŸš€ ML ì„ë² ë”© ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
            logger.info("ğŸ’¡ íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL êµ¬ë¬¸ ì˜¤ë¥˜ ì™„ì „ í•´ê²°!")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ - ì´ì „ì— ì‹¤íŒ¨í–ˆë˜ í…ìŠ¤íŠ¸ë“¤ í¬í•¨
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?",
                "Startup advice for new founders",
                "PhD vs startup career path"
            ]
            
            results = []
            for i, query in enumerate(test_queries, 1):
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
                
                try:
                    result = self.retrieve_and_generate(query)
                    results.append(result)
                    
                    if "error" in result:
                        logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                        continue
                        
                except Exception as e:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    results.append({"query": query, "error": str(e)})
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_ml_embedding_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            successful_queries = sum(1 for r in results if "error" not in r)
            
            logger.info("âœ… ML ì„ë² ë”© ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ì„±ê³µ: {successful_queries}/{len(test_queries)} ì¿¼ë¦¬")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return successful_queries > 0
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', 
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = RAGPipelineWithMLEmbedding(project_id, dataset_id)
        success = pipeline.run_full_pipeline()
        
        if success:
            print("ğŸ‰ ML ì„ë² ë”© ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
            print("âœ… íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL êµ¬ë¬¸ ì˜¤ë¥˜ ì™„ì „ í•´ê²°!")
            print("ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            print("ğŸ’¡ BigQuery ML APIì˜ ì„ë² ë”© ìƒì„± ê¸°ëŠ¥ì„ í™œìš©í•œ "
                  "í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤!")
        else:
            print("âŒ ML ì„ë² ë”© ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main()) 