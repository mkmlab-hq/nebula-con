#!/usr/bin/env python3
"""
RAG íŒŒì´í”„ë¼ì¸ - Vertex AI ì§ì ‘ í˜¸ì¶œ ë°©ì‹
BigQuery ML API ë¬¸ì œë¥¼ ìš°íšŒí•˜ì—¬ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŒ€ì•ˆ êµ¬í˜„
"""

import json
import logging
import os
from typing import Any, Dict, List
from google.cloud import bigquery
from google.cloud import aiplatform
import vertexai
from vertexai.language_models import TextGenerationModel
from vertexai.vision_models import MultiModalEmbeddingModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineVertexAI:
    """Vertex AI ì§ì ‘ í˜¸ì¶œ ë°©ì‹ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, project_id: str, dataset_id: str, location: str = "us-central1"):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.location = location
        
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client()
        
        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=location)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = MultiModalEmbeddingModel.from_pretrained("textembedding-gecko@003")
        self.text_model = TextGenerationModel.from_pretrained("gemini-pro")
        
        logger.info("âœ… Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}, ë¦¬ì „: {location}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            logger.info(f"ğŸ” ì„ë² ë”© ìƒì„± ì¤‘: {text[:50]}...")
            
            # Vertex AI ì§ì ‘ í˜¸ì¶œ
            embeddings = self.embedding_model.get_embeddings([text])
            embedding = embeddings[0].values
            
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding)}ì°¨ì›")
            return embedding
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate_text(self, prompt: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            logger.info(f"ğŸ” í…ìŠ¤íŠ¸ ìƒì„± ì¤‘: {prompt[:50]}...")
            
            # Vertex AI ì§ì ‘ í˜¸ì¶œ
            response = self.text_model.predict(prompt)
            answer = response.text
            
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def search_similar_documents(self, query_embedding: List[float], 
                                embeddings_table: str = "hacker_news_embeddings_external",
                                top_k: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ - ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸” í™œìš©"""
        try:
            logger.info("ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
            search_query = f"""
            SELECT id, title, text, combined_text
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            LIMIT {top_k}
            """
            
            result = self.bq_client.query(search_query)
            rows = list(result.result())
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ (ì„ì‹œ êµ¬í˜„)
            filtered_results = []
            for row in rows:
                if row.text and any(keyword in row.text.lower() for keyword in 
                                  ['ai', 'machine learning', 'data science', 'startup']):
                    filtered_results.append({
                        'id': row.id,
                        'title': row.title,
                        'text': row.text,
                        'combined_text': row.combined_text
                    })
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ë¬¸ì„œ")
            return filtered_results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìƒ˜í”Œ ë°˜í™˜
            return [{'id': 1, 'title': 'Sample', 'text': 'Sample text for testing'}]
    
    def retrieve_and_generate(self, query_text: str, top_k: int = 5) -> Dict[str, Any]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            # 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_embedding(query_text)
            
            # 2. ìœ ì‚¬ë„ ê²€ìƒ‰
            search_results = self.search_similar_documents(query_embedding, top_k=top_k)
            
            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n".join([
                f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['text'][:200]}..."
                for doc in search_results
            ])
            
            # 4. AI ë‹µë³€ ìƒì„±
            prompt = f"""
            ë‹¤ìŒ HackerNews ê²Œì‹œê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {query_text}
            
            ì°¸ê³  ìë£Œ:
            {context}
            
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            answer = self.generate_text(prompt)
            
            return {
                "query": query_text,
                "context": context,
                "answer": answer,
                "sources": [
                    {
                        "id": doc['id'],
                        "title": doc['title'],
                        "text_preview": doc['text'][:100] + "..." if doc['text'] else "ë‚´ìš© ì—†ìŒ"
                    }
                    for doc in search_results
                ]
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            logger.info("ğŸš€ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?",
                "Startup advice for new founders",
                "PhD vs startup career path"
            ]
            
            results = []
            for i, query in enumerate(test_queries, 1):
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
                
                try:
                    result = self.retrieve_and_generate(query)
                    results.append(result)
                    
                    if "error" in result:
                        logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                        # ê°œë³„ ì¿¼ë¦¬ ì‹¤íŒ¨ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
                        continue
                        
                except Exception as e:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    results.append({"query": query, "error": str(e)})
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_vertex_ai_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            successful_queries = sum(1 for r in results if "error" not in r)
            
            logger.info("âœ… Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ì„±ê³µ: {successful_queries}/{len(test_queries)} ì¿¼ë¦¬")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return successful_queries > 0
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT',
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')
        location = os.environ.get('GOOGLE_CLOUD_LOCATION', 'us-central1')
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = RAGPipelineVertexAI(project_id, dataset_id, location)
        success = pipeline.run_full_pipeline()
        
        if success:
            print("ğŸ‰ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
            print("ê²°ê³¼ íŒŒì¼: rag_pipeline_vertex_ai_results.json")
            print("ğŸ“Š ë‹µë³€ ìƒ˜í”Œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        else:
            print("âŒ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main()) 