#!/usr/bin/env python3
"""
RAG íŒŒì´í”„ë¼ì¸ - Vertex AI ì§ì ‘ í˜¸ì¶œ ë°©ì‹ (ìˆ˜ì •ëœ ë²„ì „)
ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
"""

import json
import logging
import os
from typing import Any, Dict, List
from google.cloud import bigquery
import vertexai
from vertexai.language_models import TextGenerationModel
from vertexai.generative_models import GenerativeModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineVertexAIFixed:
    """Vertex AI ì§ì ‘ í˜¸ì¶œ ë°©ì‹ RAG íŒŒì´í”„ë¼ì¸ (ìˆ˜ì •ëœ ë²„ì „)"""
    
    def __init__(self, project_id: str, dataset_id: str, location: str = "us-central1"):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.location = location
        
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client()
        
        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=location)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©)
        self.text_model = GenerativeModel("gemini-1.5-flash")
        
        logger.info("âœ… Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}, ë¦¬ì „: {location}")
    
    def generate_text(self, prompt: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            logger.info(f"ğŸ” í…ìŠ¤íŠ¸ ìƒì„± ì¤‘: {prompt[:50]}...")
            
            # Vertex AI ì§ì ‘ í˜¸ì¶œ
            response = self.text_model.generate_content(prompt)
            answer = response.text
            
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def search_documents(self, query_text: str, 
                        embeddings_table: str = "hacker_news_embeddings_external",
                        top_k: int = 5) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰ - í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§"""
        try:
            logger.info("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
            search_query = f"""
            SELECT id, title, text, combined_text
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            LIMIT {top_k * 2}
            """
            
            result = self.bq_client.query(search_query)
            rows = list(result.result())
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
            keywords = query_text.lower().split()
            filtered_results = []
            
            for row in rows:
                if row.text:
                    text_lower = row.text.lower()
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    score = sum(1 for keyword in keywords if keyword in text_lower)
                    if score > 0:
                        filtered_results.append({
                            'id': row.id,
                            'title': row.title,
                            'text': row.text,
                            'combined_text': row.combined_text,
                            'score': score
                        })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            filtered_results.sort(key=lambda x: x['score'], reverse=True)
            top_results = filtered_results[:top_k]
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(top_results)}ê°œ ë¬¸ì„œ")
            return top_results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìƒ˜í”Œ ë°˜í™˜
            return [{'id': 1, 'title': 'Sample', 'text': 'Sample text for testing'}]
    
    def retrieve_and_generate(self, query_text: str, top_k: int = 5) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_documents(query_text, top_k=top_k)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n".join([
                f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['text'][:200]}..."
                for doc in search_results
            ])
            
            # 3. AI ë‹µë³€ ìƒì„±
            prompt = f"""
            ë‹¤ìŒ HackerNews ê²Œì‹œê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {query_text}
            
            ì°¸ê³  ìë£Œ:
            {context}
            
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            answer = self.generate_text(prompt)
            
            return {
                "query": query_text,
                "context": context,
                "answer": answer,
                "sources": [
                    {
                        "id": doc['id'],
                        "title": doc['title'],
                        "text_preview": doc['text'][:100] + "..." if doc['text'] else "ë‚´ìš© ì—†ìŒ",
                        "relevance_score": doc.get('score', 0)
                    }
                    for doc in search_results
                ]
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - Vertex AI ì§ì ‘ í˜¸ì¶œ"""
        try:
            logger.info("ğŸš€ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?",
                "Startup advice for new founders",
                "PhD vs startup career path"
            ]
            
            results = []
            for i, query in enumerate(test_queries, 1):
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
                
                try:
                    result = self.retrieve_and_generate(query)
                    results.append(result)
                    
                    if "error" in result:
                        logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                        continue
                        
                except Exception as e:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    results.append({"query": query, "error": str(e)})
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_vertex_ai_fixed_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            successful_queries = sum(1 for r in results if "error" not in r)
            
            logger.info("âœ… Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ì„±ê³µ: {successful_queries}/{len(test_queries)} ì¿¼ë¦¬")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return successful_queries > 0
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT',
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')
        location = os.environ.get('GOOGLE_CLOUD_LOCATION', 'us-central1')
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = RAGPipelineVertexAIFixed(project_id, dataset_id, location)
        success = pipeline.run_full_pipeline()
        
        if success:
            print("ğŸ‰ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
            print("ê²°ê³¼ íŒŒì¼: rag_pipeline_vertex_ai_fixed_results.json")
            print("ğŸ“Š ë‹µë³€ ìƒ˜í”Œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        else:
            print("âŒ Vertex AI ì§ì ‘ í˜¸ì¶œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main()) 