#!/usr/bin/env python3
"""
ì‹¤ì œ BigQuery í”„ë¡œì íŠ¸ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì…‹ê³¼ í…Œì´ë¸” í™•ì¸
ì •í™•í•œ ë°ì´í„°ì…‹ IDë¥¼ ì°¾ê¸° ìœ„í•œ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
"""

import logging
from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_project_structure():
    """í”„ë¡œì íŠ¸ êµ¬ì¡° ì „ì²´ í™•ì¸"""
    try:
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = bigquery.Client(project='persona-diary-service', location='US')
        
        logger.info("ğŸ” í”„ë¡œì íŠ¸ persona-diary-service êµ¬ì¡° í™•ì¸ ì¤‘...")
        
        # 1. ëª¨ë“  ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ
        datasets = list(client.list_datasets())
        logger.info(f"âœ… ë°œê²¬ëœ ë°ì´í„°ì…‹ ìˆ˜: {len(datasets)}")
        
        for dataset in datasets:
            logger.info(f"ğŸ“ ë°ì´í„°ì…‹: {dataset.dataset_id}")
            
            # 2. ê° ë°ì´í„°ì…‹ì˜ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            try:
                tables = list(client.list_tables(dataset.dataset_id))
                logger.info(f"   ğŸ“‹ í…Œì´ë¸” ìˆ˜: {len(tables)}")
                
                for table in tables:
                    logger.info(f"      ğŸ—ƒï¸ í…Œì´ë¸”: {table.table_id}")
                    
                    # 3. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
                    try:
                        table_ref = client.dataset(dataset.dataset_id).table(table.table_id)
                        table_obj = client.get_table(table_ref)
                        
                        if table_obj.schema:
                            logger.info(f"         ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(table_obj.schema)}")
                            for field in table_obj.schema[:3]:  # ì²˜ìŒ 3ê°œ ì»¬ëŸ¼ë§Œ
                                logger.info(f"            - {field.name}: {field.field_type}")
                            if len(table_obj.schema) > 3:
                                logger.info(f"            ... (ì´ {len(table_obj.schema)}ê°œ ì»¬ëŸ¼)")
                        else:
                            logger.info("         ğŸ“Š ìŠ¤í‚¤ë§ˆ ì •ë³´ ì—†ìŒ")
                            
                    except Exception as e:
                        logger.error(f"         âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
                        
            except Exception as e:
                logger.error(f"   âŒ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 4. ML ëª¨ë¸ ëª©ë¡ í™•ì¸
        logger.info("\nğŸ¤– ML ëª¨ë¸ í™•ì¸ ì¤‘...")
        try:
            models = list(client.list_models())
            logger.info(f"âœ… ë°œê²¬ëœ ML ëª¨ë¸ ìˆ˜: {len(models)}")
            
            for model in models:
                logger.info(f"   ğŸ§  ëª¨ë¸: {model.model_id}")
                if hasattr(model, 'model_type'):
                    logger.info(f"      íƒ€ì…: {model.model_type}")
                if hasattr(model, 'remote_service_type'):
                    logger.info(f"      ì„œë¹„ìŠ¤: {model.remote_service_type}")
                    
        except Exception as e:
            logger.error(f"âŒ ML ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 5. ì—°ê²° ëª©ë¡ í™•ì¸
        logger.info("\nğŸ”— ì—°ê²° í™•ì¸ ì¤‘...")
        try:
            connections = list(client.list_connections())
            logger.info(f"âœ… ë°œê²¬ëœ ì—°ê²° ìˆ˜: {len(connections)}")
            
            for conn in connections:
                logger.info(f"   ğŸ”Œ ì—°ê²°: {conn.connection_id}")
                if hasattr(conn, 'connection_type'):
                    logger.info(f"      íƒ€ì…: {conn.connection_type}")
                    
        except Exception as e:
            logger.error(f"âŒ ì—°ê²° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ ì‹¤íŒ¨: {e}")


def check_specific_dataset(dataset_id):
    """íŠ¹ì • ë°ì´í„°ì…‹ ìƒì„¸ í™•ì¸"""
    try:
        client = bigquery.Client(project='persona-diary-service', location='US')
        
        logger.info(f"ğŸ” ë°ì´í„°ì…‹ {dataset_id} ìƒì„¸ í™•ì¸ ì¤‘...")
        
        # ë°ì´í„°ì…‹ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        dataset_ref = client.dataset(dataset_id)
        dataset = client.get_dataset(dataset_ref)
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ {dataset_id} ì¡´ì¬ í™•ì¸")
        logger.info(f"   ğŸ“ ìœ„ì¹˜: {dataset.location}")
        logger.info(f"   ğŸ“… ìƒì„±ì¼: {dataset.created}")
        
        # í…Œì´ë¸” ëª©ë¡
        tables = list(client.list_tables(dataset_id))
        logger.info(f"   ğŸ“‹ í…Œì´ë¸” ìˆ˜: {len(tables)}")
        
        for table in tables:
            logger.info(f"      ğŸ—ƒï¸ í…Œì´ë¸”: {table.table_id}")
            
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ {dataset_id} í™•ì¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    # ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
    check_project_structure()
    
    # íŠ¹ì • ë°ì´í„°ì…‹ë“¤ í™•ì¸ ì‹œë„
    test_datasets = [
        'nebula_con',
        'nebula_con_kaggle', 
        'kaggle',
        'hacker_news',
        'default'
    ]
    
    logger.info("\nğŸ” íŠ¹ì • ë°ì´í„°ì…‹ ì¡´ì¬ ì—¬ë¶€ í™•ì¸...")
    for dataset_id in test_datasets:
        try:
            check_specific_dataset(dataset_id)
        except Exception as e:
            logger.info(f"   âŒ ë°ì´í„°ì…‹ {dataset_id} ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    
    logger.info("\nâœ… í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸ ì™„ë£Œ!") 