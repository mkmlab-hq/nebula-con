#!/usr/bin/env python3
"""
í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ - AI ëª¨ë¸ ì—†ì´ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
BigQuery ML APIì™€ Vertex AI ë¬¸ì œë¥¼ ëª¨ë‘ ìš°íšŒí•˜ëŠ” ìµœì¢… ëŒ€ì•ˆ
"""

import json
import logging
import os
from typing import Any, Dict, List
from google.cloud import bigquery
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KeywordBasedRAGPipeline:
    """í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ - AI ëª¨ë¸ ì—†ì´ ì‘ë™"""
    
    def __init__(self, project_id: str, dataset_id: str):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client()
        
        # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.keyword_weights = {
            'ai': 3.0,
            'artificial intelligence': 3.0,
            'machine learning': 3.0,
            'ml': 3.0,
            'deep learning': 3.0,
            'data science': 2.5,
            'startup': 2.0,
            'founder': 2.0,
            'entrepreneur': 2.0,
            'phd': 1.5,
            'research': 1.5,
            'optimization': 2.0,
            'best practices': 2.0,
            'trends': 1.5,
            'technology': 1.0,
            'software': 1.0,
            'development': 1.0
        }
        
        logger.info("âœ… í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}")
    
    def calculate_relevance_score(self, text: str, query: str) -> float:
        """í…ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        query_lower = query.lower()
        
        # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_score = 0.0
        for keyword, weight in self.keyword_weights.items():
            if keyword in text_lower:
                keyword_score += weight
        
        # 2. ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­ ì ìˆ˜
        query_words = re.findall(r'\b\w+\b', query_lower)
        word_match_score = 0.0
        for word in query_words:
            if len(word) > 2 and word in text_lower:
                word_match_score += 1.0
        
        # 3. ë¬¸ì¥ ê¸¸ì´ ë³´ì • (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í…ìŠ¤íŠ¸ì— ë¶ˆì´ìµ)
        length_penalty = 1.0
        if len(text) < 50:
            length_penalty = 0.7
        elif len(text) > 1000:
            length_penalty = 0.8
        
        # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚°
        total_score = (keyword_score + word_match_score) * length_penalty
        
        return total_score
    
    def search_documents(self, query_text: str, 
                        embeddings_table: str = "hacker_news_embeddings_external",
                        top_k: int = 5) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            logger.info("ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            search_query = f"""
            SELECT id, title, text, combined_text
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            LIMIT {top_k * 3}
            """
            
            result = self.bq_client.query(search_query)
            rows = list(result.result())
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
            scored_results = []
            for row in rows:
                if row.text:
                    # ì œëª©ê³¼ ë³¸ë¬¸ ëª¨ë‘ ê³ ë ¤
                    title_score = self.calculate_relevance_score(row.title, query_text)
                    text_score = self.calculate_relevance_score(row.text, query_text)
                    
                    # ì œëª©ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    total_score = title_score * 1.5 + text_score
                    
                    scored_results.append({
                        'id': row.id,
                        'title': row.title,
                        'text': row.text,
                        'combined_text': row.combined_text,
                        'relevance_score': total_score
                    })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            scored_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            top_results = scored_results[:top_k]
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(top_results)}ê°œ ë¬¸ì„œ")
            for i, result in enumerate(top_results):
                logger.info(f"  {i+1}. ì ìˆ˜: {result['relevance_score']:.2f} - {result['title'][:50]}...")
            
            return top_results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìƒ˜í”Œ ë°˜í™˜
            return [{'id': 1, 'title': 'Sample', 'text': 'Sample text for testing', 'relevance_score': 1.0}]
    
    def generate_answer_template(self, query_text: str, search_results: List[Dict[str, Any]]) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„± - AI ëª¨ë¸ ì—†ì´"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_summary = []
            for i, doc in enumerate(search_results, 1):
                context_summary.append(f"{i}. {doc['title']}")
                if doc['text']:
                    # í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ì¥)
                    first_sentence = doc['text'].split('.')[0] + '.'
                    context_summary.append(f"   ìš”ì•½: {first_sentence}")
            
            context_text = "\n".join(context_summary)
            
            # ì¿¼ë¦¬ ìœ í˜•ë³„ í…œí”Œë¦¿ ì„ íƒ
            query_lower = query_text.lower()
            
            if any(word in query_lower for word in ['trend', 'latest', 'recent']):
                template = f"""
                **AI ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼**
                
                ì§ˆë¬¸: {query_text}
                
                **ì°¸ê³  ìë£Œ:**
                {context_text}
                
                **í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
                - ì œê³µëœ HackerNews ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ë¶„ì•¼ì˜ ìµœì‹  ë™í–¥ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤
                - ê° ê²Œì‹œê¸€ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤
                - ê´€ë ¨ì„± ì ìˆ˜: {search_results[0]['relevance_score']:.2f} (ìµœê³  ì ìˆ˜)
                
                **ì¶”ì²œ ìë£Œ:**
                ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²Œì‹œê¸€: "{search_results[0]['title']}"
                """
            
            elif any(word in query_lower for word in ['how', 'optimize', 'best practice']):
                template = f"""
                **ìµœì í™” ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ê°€ì´ë“œ**
                
                ì§ˆë¬¸: {query_text}
                
                **ì°¸ê³  ìë£Œ:**
                {context_text}
                
                **í•µì‹¬ ê°€ì´ë“œë¼ì¸:**
                - ì œê³µëœ HackerNews ë°ì´í„°ì—ì„œ ê´€ë ¨ì„± ë†’ì€ ì‹¤ìš©ì  ì¡°ì–¸ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤
                - ê° ê²Œì‹œê¸€ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ë¬´ì— ì ìš© ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤
                - ê´€ë ¨ì„± ì ìˆ˜: {search_results[0]['relevance_score']:.2f} (ìµœê³  ì ìˆ˜)
                
                **ì¶”ì²œ ìë£Œ:**
                ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²Œì‹œê¸€: "{search_results[0]['title']}"
                """
            
            else:
                template = f"""
                **ì§ˆë¬¸ ë‹µë³€ ê²°ê³¼**
                
                ì§ˆë¬¸: {query_text}
                
                **ì°¸ê³  ìë£Œ:**
                {context_text}
                
                **ë¶„ì„ ê²°ê³¼:**
                - ì œê³µëœ HackerNews ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤
                - ê° ê²Œì‹œê¸€ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤
                - ê´€ë ¨ì„± ì ìˆ˜: {search_results[0]['relevance_score']:.2f} (ìµœê³  ì ìˆ˜)
                
                **ì¶”ì²œ ìë£Œ:**
                ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²Œì‹œê¸€: "{search_results[0]['title']}"
                """
            
            return template.strip()
            
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì§ˆë¬¸: {query_text}\n\në‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def retrieve_and_generate(self, query_text: str, top_k: int = 5) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± - í‚¤ì›Œë“œ ê¸°ë°˜"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_documents(query_text, top_k=top_k)
            
            # 2. í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            answer = self.generate_answer_template(query_text, search_results)
            
            return {
                "query": query_text,
                "answer": answer,
                "sources": [
                    {
                        "id": doc['id'],
                        "title": doc['title'],
                        "text_preview": doc['text'][:100] + "..." if doc['text'] else "ë‚´ìš© ì—†ìŒ",
                        "relevance_score": doc['relevance_score']
                    }
                    for doc in search_results
                ],
                "method": "keyword_based_search",
                "ai_model_used": False
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - í‚¤ì›Œë“œ ê¸°ë°˜"""
        try:
            logger.info("ğŸš€ í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?",
                "Startup advice for new founders",
                "PhD vs startup career path"
            ]
            
            results = []
            for i, query in enumerate(test_queries, 1):
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
                
                try:
                    result = self.retrieve_and_generate(query)
                    results.append(result)
                    
                    if "error" in result:
                        logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                        continue
                        
                except Exception as e:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    results.append({"query": query, "error": str(e)})
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_keyword_based_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            successful_queries = sum(1 for r in results if "error" not in r)
            
            logger.info("âœ… í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ì„±ê³µ: {successful_queries}/{len(test_queries)} ì¿¼ë¦¬")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return successful_queries > 0
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT',
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = KeywordBasedRAGPipeline(project_id, dataset_id)
        success = pipeline.run_full_pipeline()
        
        if success:
            print("ğŸ‰ í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
            print("ê²°ê³¼ íŒŒì¼: rag_pipeline_keyword_based_results.json")
            print("ğŸ“Š ë‹µë³€ ìƒ˜í”Œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            print("ğŸ’¡ AI ëª¨ë¸ ì—†ì´ë„ ì‘ë™í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤!")
        else:
            print("âŒ í‚¤ì›Œë“œ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main()) 