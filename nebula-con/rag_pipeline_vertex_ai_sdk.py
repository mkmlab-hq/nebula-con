#!/usr/bin/env python3
"""
Vertex AI Python SDKë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸
BigQuery ML ìš°íšŒí•˜ì—¬ ì™„ì „í•œ AI ê¸°ë°˜ RAG êµ¬í˜„
"""

import json
import logging
from typing import Any, Dict, List
import numpy as np
from google.cloud import bigquery
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel, TextGenerationModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineVertexAISDK:
    """Vertex AI Python SDKë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸"""

    def __init__(self, project_id: str = 'persona-diary-service',
                 dataset_id: str = 'nebula_con_kaggle',
                 location: str = 'us-central1'):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.location = location

        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client(
            project=project_id, location=location
        )

        # Vertex AI ì´ˆê¸°í™”
        aiplatform.init(project=project_id, location=location)

        # Vertex AI ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")
        self.generation_model = TextGenerationModel.from_pretrained("gemini-1.5-flash-001")

        logger.info(
            f"ğŸš€ Vertex AI SDK RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ: "
            f"{project_id}.{dataset_id}"
        )
        logger.info(f"ğŸ“ ìœ„ì¹˜: {location}")
        logger.info(f"ğŸ§  ì„ë² ë”© ëª¨ë¸: text-embedding-004")
        logger.info(f"ğŸ§  ìƒì„± ëª¨ë¸: gemini-1.5-flash-001")

    def load_documents_from_bigquery(self) -> List[Dict[str, Any]]:
        """BigQueryì—ì„œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            table_name = 'hacker_news_embeddings_external'
            
            query = f"""
            SELECT id, title, text
            FROM `{self.project_id}.{self.dataset_id}.{table_name}`
            WHERE text IS NOT NULL OR title IS NOT NULL
            LIMIT 100
            """
            
            logger.info(f"ğŸ“Š BigQueryì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘: {query[:100]}...")
            
            result = self.bq_client.query(query)
            rows = list(result.result())
            
            documents = []
            for row in rows:
                if row.text or row.title:
                    combined_text = f"{row.title or ''} {row.text or ''}".strip()
                    documents.append({
                        'id': row.id,
                        'title': row.title,
                        'text': row.text,
                        'combined_text': combined_text
                    })
            
            logger.info(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            logger.info(f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            
            embeddings = []
            for text in texts:
                if text and text.strip():
                    embedding = self.embedding_model.get_embeddings([text])
                    embeddings.append(embedding[0].values)
                else:
                    # ë¹ˆ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë”ë¯¸ ì„ë² ë”©
                    embeddings.append([0.0] * 768)
            
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def calculate_cosine_similarity(self, query_embedding: List[float], 
                                   doc_embeddings: List[List[float]]) -> List[float]:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            query_vec = np.array(query_embedding)
            similarities = []
            
            for doc_embedding in doc_embeddings:
                doc_vec = np.array(doc_embedding)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                dot_product = np.dot(query_vec, doc_vec)
                norm_query = np.linalg.norm(query_vec)
                norm_doc = np.linalg.norm(doc_vec)
                
                if norm_query > 0 and norm_doc > 0:
                    similarity = dot_product / (norm_query * norm_doc)
                else:
                    similarity = 0.0
                
                similarities.append(similarity)
            
            return similarities
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [0.0] * len(doc_embeddings)

    def search_similar_documents(self, query: str, documents: List[Dict[str, Any]], 
                                top_k: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            logger.info(f"ğŸ” ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘: {query}")
            
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_embeddings([query])
            if not query_embedding:
                logger.warning("âš ï¸ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # 2. ë¬¸ì„œ ì„ë² ë”© ìƒì„±
            doc_texts = [doc['combined_text'] for doc in documents]
            doc_embeddings = self.generate_embeddings(doc_texts)
            if not doc_embeddings:
                logger.warning("âš ï¸ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # 3. ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.calculate_cosine_similarity(query_embedding[0], doc_embeddings)
            
            # 4. ìœ ì‚¬ë„ ì ìˆ˜ì™€ ë¬¸ì„œ ê²°í•©
            scored_docs = []
            for i, doc in enumerate(documents):
                scored_docs.append({
                    **doc,
                    'similarity_score': similarities[i]
                })
            
            # 5. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            scored_docs.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            # 6. ìƒìœ„ kê°œ ë°˜í™˜
            top_results = scored_docs[:top_k]
            
            logger.info(f"âœ… ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(top_results)}ê°œ ë¬¸ì„œ")
            return top_results
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_ai_answer(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """AI ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            if not search_results:
                return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = ""
            for i, doc in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                context += f"ë¬¸ì„œ {i+1}:\n"
                context += f"ì œëª©: {doc.get('title', 'N/A')}\n"
                context += f"ë‚´ìš©: {doc.get('text', 'N/A')[:300]}...\n"
                context += f"ìœ ì‚¬ë„ ì ìˆ˜: {doc.get('similarity_score', 0):.3f}\n\n"
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ì»¨í…ìŠ¤íŠ¸:
{context}

ìš”êµ¬ì‚¬í•­:
1. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ë‹µë³€ ëì— "ì´ ë‹µë³€ì€ Vertex AIì˜ text-embedding-004ì™€ gemini-1.5-flash-001 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."ë¼ê³  í‘œì‹œí•˜ì„¸ìš”

ë‹µë³€:
"""
            
            logger.info("ğŸ§  AI ë‹µë³€ ìƒì„± ì¤‘...")
            
            # Vertex AIë¡œ ë‹µë³€ ìƒì„±
            response = self.generation_model.predict(prompt)
            answer = response.text
            
            logger.info("âœ… AI ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ AI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def run_rag_pipeline(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: {query}")
            
            # 1. BigQueryì—ì„œ ë¬¸ì„œ ë¡œë“œ
            documents = self.load_documents_from_bigquery()
            if not documents:
                return {
                    'query': query,
                    'search_results': [],
                    'answer': 'ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'status': 'error',
                    'error': 'ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨'
                }
            
            # 2. ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_similar_documents(query, documents, top_k)
            if not search_results:
                return {
                    'query': query,
                    'search_results': [],
                    'answer': f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'status': 'no_results'
                }
            
            # 3. AI ê¸°ë°˜ ë‹µë³€ ìƒì„±
            answer = self.generate_ai_answer(query, search_results)
            
            result = {
                'query': query,
                'search_results': search_results,
                'answer': answer,
                'status': 'success',
                'search_method': 'vertex_ai_embedding_similarity',
                'location': self.location,
                'models_used': ['text-embedding-004', 'gemini-1.5-flash-001']
            }
            
            logger.info(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {query}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return {
                'query': query,
                'search_results': [],
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'status': 'error',
                'error': str(e)
            }

    def run_full_pipeline_test(self, test_queries: List[str]) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Vertex AI SDK RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        
        results = []
        success_count = 0
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
            
            try:
                result = self.run_rag_pipeline(query)
                results.append(result)
                
                if result['status'] == 'success':
                    success_count += 1
                    logger.info(f"âœ… ì¿¼ë¦¬ {i} ì„±ê³µ")
                else:
                    logger.warning(f"âš ï¸ ì¿¼ë¦¬ {i} ì‹¤íŒ¨: {result.get('status', 'unknown')}")
                    
            except Exception as e:
                logger.error(f"âŒ ì¿¼ë¦¬ {i} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                results.append({
                    'query': query,
                    'search_results': [],
                    'answer': f"ì˜ˆì™¸ ë°œìƒ: {str(e)}",
                    'status': 'exception',
                    'error': str(e)
                })
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            'total_queries': len(test_queries),
            'successful_queries': success_count,
            'success_rate': f"{success_count}/{len(test_queries)}",
            'pipeline_type': 'vertex_ai_sdk_direct',
            'models_used': ['text-embedding-004', 'gemini-1.5-flash-001'],
            'results': results
        }
        
        # ê²°ê³¼ ì €ì¥
        output_file = 'vertex_ai_sdk_rag_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… Vertex AI SDK RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ì„±ê³µ: {success_count}/{len(test_queries)} ì¿¼ë¦¬")
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ëŠ” ì„¤ì •
    project_id = "persona-diary-service"
    dataset_id = "nebula_con_kaggle"
    location = "us-central1"
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag_pipeline = RAGPipelineVertexAISDK(project_id, dataset_id, location)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "How to optimize machine learning models?",
        "Best practices for data science projects?",
        "Startup advice for new founders",
        "PhD vs startup career path",
        "Machine learning in production"
    ]
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = rag_pipeline.run_full_pipeline_test(test_queries)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ Vertex AI SDK RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
    print(f"âœ… BigQuery ML ìš°íšŒí•˜ì—¬ ì§ì ‘ Vertex AI ì‚¬ìš©!")
    print(f"âœ… ì™„ì „í•œ AI ê¸°ë°˜ ì„ë² ë”© ë° ë‹µë³€ ìƒì„±!")
    print(f"ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")


if __name__ == "__main__":
    main() 