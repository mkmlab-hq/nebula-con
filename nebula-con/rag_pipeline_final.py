#!/usr/bin/env python3
"""
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ - ìµœì¢… ë²„ì „
ê¸°ì¡´ í…Œì´ë¸” í™œìš©í•˜ì—¬ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
"""

import json
import logging
import os
from typing import Any, Dict

from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineFinal:
    """RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… í´ë˜ìŠ¤ - ê¸°ì¡´ í…Œì´ë¸” í™œìš©"""

    def __init__(self, project_id: str, dataset_id: str):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.client = bigquery.Client()
        
        logger.info("âœ… RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… ë²„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}")

    def retrieve_and_generate(self, query_text: str,
                             embeddings_table: str = "hacker_news_embeddings_external",
                             top_k: int = 5) -> Dict[str, Any]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± - ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸” í™œìš©"""
        try:
            # 1. ê¸°ì¡´ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì‹œì‘ (ë¹„ìš© ì ˆì•½)
            search_query = f"""
            SELECT 
                id,
                title,
                text,
                combined_text
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            WHERE 
                LOWER(combined_text) LIKE LOWER('%{query_text.lower()}%')
                OR LOWER(title) LIKE LOWER('%{query_text.lower()}%')
            ORDER BY id DESC
            LIMIT {top_k}
            """
            
            logger.info("ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            
            search_job = self.client.query(search_query)
            search_results = list(search_job.result())
            
            if not search_results:
                logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í…Œì´ë¸”ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ")
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…Œì´ë¸”ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
                sample_query = f"""
                SELECT id, title, text, combined_text
                FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
                LIMIT {top_k}
                """
                search_job = self.client.query(sample_query)
                search_results = list(search_job.result())
            
            # 2. AI ë‹µë³€ ìƒì„±
            context = "\n".join([
                f"ì œëª©: {row.title}\në‚´ìš©: {row.text[:200]}..."
                for row in search_results
            ])
            
            ai_query = f"""
            SELECT 
                AI.GENERATE_TEXT(
                    'ë‹¤ìŒ HackerNews ê²Œì‹œê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. '
                    'ì§ˆë¬¸: {query_text}\n\n'
                    'ì°¸ê³  ìë£Œ:\n{context}',
                    'gemini-pro'
                ) AS answer
            LIMIT 1
            """
            
            logger.info("ğŸ” AI ë‹µë³€ ìƒì„± ì¤‘...")
            ai_job = self.client.query(ai_query)
            ai_result = list(ai_job.result())
            
            if not ai_result:
                raise ValueError("AI ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
            
            answer = ai_result[0].answer
            
            return {
                "query": query_text,
                "context": context,
                "answer": answer,
                "sources": [
                    {
                        "id": row.id,
                        "title": row.title,
                        "text_preview": row.text[:100] + "..." if row.text else "ë‚´ìš© ì—†ìŒ"
                    }
                    for row in search_results
                ]
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    def run_full_pipeline(self) -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ê¸°ì¡´ í…Œì´ë¸” í™œìš©"""
        try:
            logger.info("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… ì‹¤í–‰ ì‹œì‘...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?",
                "Startup advice for new founders",
                "PhD vs startup career path"
            ]
            
            results = []
            for i, query in enumerate(test_queries, 1):
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
                result = self.retrieve_and_generate(query)
                results.append(result)
                
                if "error" in result:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                    # ê°œë³„ ì¿¼ë¦¬ ì‹¤íŒ¨ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_final_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ì„±ê³µí•œ ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            successful_queries = sum(1 for r in results if "error" not in r)
            
            logger.info("âœ… RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ì„±ê³µ: {successful_queries}/{len(test_queries)} ì¿¼ë¦¬")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return successful_queries > 0
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT',
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = RAGPipelineFinal(project_id, dataset_id)
        success = pipeline.run_full_pipeline()
        
        if success:
            print("ğŸ‰ RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… ì‹¤í–‰ ì„±ê³µ!")
            print("ê²°ê³¼ íŒŒì¼: rag_pipeline_final_results.json")
            print("ğŸ“Š ë‹µë³€ ìƒ˜í”Œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ RAG íŒŒì´í”„ë¼ì¸ ìµœì¢… ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main()) 