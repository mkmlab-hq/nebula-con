#!/usr/bin/env python3
"""
BigQuery MLì„ ì‚¬ìš©í•˜ëŠ” ìµœì¢… RAG íŒŒì´í”„ë¼ì¸
ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ embedding_model_test ëª¨ë¸ ì‚¬ìš©
"""

import json
import logging
from typing import Any, Dict, List
import numpy as np
from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipelineBigQueryMLFinal:
    """BigQuery MLì„ ì‚¬ìš©í•˜ëŠ” ìµœì¢… RAG íŒŒì´í”„ë¼ì¸"""

    def __init__(self, project_id: str = 'persona-diary-service',
                 dataset_id: str = 'nebula_con_kaggle',
                 location: str = 'us-central1'):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.location = location

        # BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bq_client = bigquery.Client(
            project=project_id, location=location
        )

        # BigQuery ML ëª¨ë¸ ê²½ë¡œ
        self.embedding_model = f"{project_id}.{dataset_id}.embedding_model_test"

        logger.info(
            f"ğŸš€ BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ: "
            f"{project_id}.{dataset_id}"
        )
        logger.info(f"ğŸ“ ìœ„ì¹˜: {location}")
        logger.info(f"ğŸ§  ì„ë² ë”© ëª¨ë¸: {self.embedding_model}")

    def load_documents_from_bigquery(self) -> List[Dict[str, Any]]:
        """BigQueryì—ì„œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            table_name = 'hacker_news_embeddings_external'
            
            query = f"""
            SELECT id, title, text
            FROM `{self.project_id}.{self.dataset_id}.{table_name}`
            WHERE text IS NOT NULL OR title IS NOT NULL
            LIMIT 50
            """
            
            logger.info(f"ğŸ“Š BigQueryì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘: {query[:100]}...")
            
            result = self.bq_client.query(query)
            rows = list(result.result())
            
            documents = []
            for row in rows:
                if row.text or row.title:
                    combined_text = f"{row.title or ''} {row.text or ''}".strip()
                    documents.append({
                        'id': row.id,
                        'title': row.title,
                        'text': row.text,
                        'combined_text': combined_text
                    })
            
            logger.info(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def generate_embeddings_bigquery_ml(self, texts: List[str]) -> List[List[float]]:
        """BigQuery MLì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            logger.info(f"ğŸ§  BigQuery MLë¡œ ì„ë² ë”© ìƒì„± ì¤‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            
            # BigQuery ML ì¿¼ë¦¬ êµ¬ì„±
            text_values = []
            for text in texts:
                if text and text.strip():
                    # SQL ì¸ì ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    safe_text = text.replace("'", "''")[:1000]  # ê¸¸ì´ ì œí•œ
                    text_values.append(f"'{safe_text}'")
            
            if not text_values:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                return []
            
            # BigQuery ML ì¿¼ë¦¬ ì‹¤í–‰
            ml_query = f"""
            SELECT
              ml_generate_embedding_result,
              content
            FROM
              ML.GENERATE_EMBEDDING(
                MODEL `{self.embedding_model}`,
                (SELECT content FROM UNNEST([{', '.join(text_values)}]) AS content)
              )
            """
            
            logger.info(f"ğŸ” BigQuery ML ì¿¼ë¦¬ ì‹¤í–‰: {ml_query[:200]}...")
            
            result = self.bq_client.query(ml_query)
            rows = list(result.result())
            
            embeddings = []
            for row in rows:
                if hasattr(row, 'ml_generate_embedding_result') and row.ml_generate_embedding_result:
                    # ì„ë² ë”© ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    embedding_values = row.ml_generate_embedding_result
                    if isinstance(embedding_values, list):
                        embeddings.append(embedding_values)
                    else:
                        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš° íŒŒì‹±
                        try:
                            import ast
                            embedding_list = ast.literal_eval(str(embedding_values))
                            embeddings.append(embedding_list)
                        except:
                            logger.warning(f"âš ï¸ ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {type(embedding_values)}")
                            embeddings.append([0.0] * 768)
                else:
                    # ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë²¡í„°
                    embeddings.append([0.0] * 768)
            
            logger.info(f"âœ… BigQuery ML ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ BigQuery ML ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def calculate_cosine_similarity(self, query_embedding: List[float], 
                                   doc_embeddings: List[List[float]]) -> List[float]:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            query_vec = np.array(query_embedding)
            similarities = []
            
            for doc_embedding in doc_embeddings:
                doc_vec = np.array(doc_embedding)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                dot_product = np.dot(query_vec, doc_vec)
                norm_query = np.linalg.norm(query_vec)
                norm_doc = np.linalg.norm(doc_vec)
                
                if norm_query > 0 and norm_doc > 0:
                    similarity = dot_product / (norm_query * norm_doc)
                else:
                    similarity = 0.0
                
                similarities.append(similarity)
            
            return similarities
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [0.0] * len(doc_embeddings)

    def search_similar_documents(self, query: str, documents: List[Dict[str, Any]], 
                                top_k: int = 5) -> List[Dict[str, Any]]:
        """BigQuery ML ì„ë² ë”©ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            logger.info(f"ğŸ” BigQuery ML ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘: {query}")
            
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_embeddings_bigquery_ml([query])
            if not query_embedding:
                logger.warning("âš ï¸ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # 2. ë¬¸ì„œ ì„ë² ë”© ìƒì„±
            doc_texts = [doc['combined_text'] for doc in documents]
            doc_embeddings = self.generate_embeddings_bigquery_ml(doc_texts)
            if not doc_embeddings:
                logger.warning("âš ï¸ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            # 3. ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.calculate_cosine_similarity(query_embedding[0], doc_embeddings)
            
            # 4. ìœ ì‚¬ë„ ì ìˆ˜ì™€ ë¬¸ì„œ ê²°í•©
            scored_docs = []
            for i, doc in enumerate(documents):
                scored_docs.append({
                    **doc,
                    'similarity_score': similarities[i]
                })
            
            # 5. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            scored_docs.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            # 6. ìƒìœ„ kê°œ ë°˜í™˜
            top_results = scored_docs[:top_k]
            
            logger.info(f"âœ… BigQuery ML ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(top_results)}ê°œ ë¬¸ì„œ")
            return top_results
            
        except Exception as e:
            logger.error(f"âŒ BigQuery ML ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_answer_template(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        if not search_results:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìƒìœ„ ê²°ê³¼ ì‚¬ìš©
        top_result = search_results[0]
        
        answer = f"""
ğŸ” **ì§ˆë¬¸**: {query}

ğŸ“š **ì°¾ì€ ì •ë³´**:
- **ì œëª©**: {top_result.get('title', 'N/A')}
- **ë‚´ìš©**: {top_result.get('text', 'N/A')[:200]}...
- **ìœ ì‚¬ë„ ì ìˆ˜**: {top_result.get('similarity_score', 0):.3f}

ğŸ’¡ **BigQuery ML í™œìš©**: ì´ ë‹µë³€ì€ BigQuery MLì˜ `{self.embedding_model}` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ğŸ“ **ìœ„ì¹˜**: {self.location} (ì˜¬ë°”ë¥¸ ìœ„ì¹˜ ì‚¬ìš©)
ğŸ§  **AI ê¸°ìˆ **: Vertex AI ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        """.strip()
        
        return answer

    def run_rag_pipeline(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸš€ BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: {query}")
            
            # 1. BigQueryì—ì„œ ë¬¸ì„œ ë¡œë“œ
            documents = self.load_documents_from_bigquery()
            if not documents:
                return {
                    'query': query,
                    'search_results': [],
                    'answer': 'ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'status': 'error',
                    'error': 'ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨'
                }
            
            # 2. BigQuery ML ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_results = self.search_similar_documents(query, documents, top_k)
            if not search_results:
                return {
                    'query': query,
                    'search_results': [],
                    'answer': f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'status': 'no_results'
                }
            
            # 3. í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            answer = self.generate_answer_template(query, search_results)
            
            result = {
                'query': query,
                'search_results': search_results,
                'answer': answer,
                'status': 'success',
                'search_method': 'bigquery_ml_embedding_similarity',
                'location': self.location,
                'embedding_model': self.embedding_model,
                'pipeline_type': 'bigquery_ml_final'
            }
            
            logger.info(f"âœ… BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {query}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return {
                'query': query,
                'search_results': [],
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'status': 'error',
                'error': str(e)
            }

    def run_full_pipeline_test(self, test_queries: List[str]) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        
        # 1. ëª¨ë¸ ìƒíƒœ í™•ì¸
        try:
            model_check_query = f"""
            SELECT model_id, model_type, remote_service_type, endpoint
            FROM `{self.project_id}.{self.dataset_id}.INFORMATION_SCHEMA.ML_MODELS`
            WHERE model_id = 'embedding_model_test'
            """
            
            model_result = self.bq_client.query(model_check_query)
            model_info = list(model_result.result())
            
            if model_info:
                logger.info(f"âœ… ëª¨ë¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {model_info[0]}")
            else:
                logger.warning("âš ï¸ ëª¨ë¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        results = []
        success_count = 0
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}/{len(test_queries)} ì‹¤í–‰: {query}")
            
            try:
                result = self.run_rag_pipeline(query)
                results.append(result)
                
                if result['status'] == 'success':
                    success_count += 1
                    logger.info(f"âœ… ì¿¼ë¦¬ {i} ì„±ê³µ")
                else:
                    logger.warning(f"âš ï¸ ì¿¼ë¦¬ {i} ì‹¤íŒ¨: {result.get('status', 'unknown')}")
                    
            except Exception as e:
                logger.error(f"âŒ ì¿¼ë¦¬ {i} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                results.append({
                    'query': query,
                    'search_results': [],
                    'answer': f"ì˜ˆì™¸ ë°œìƒ: {str(e)}",
                    'status': 'exception',
                    'error': str(e)
                })
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            'total_queries': len(test_queries),
            'successful_queries': success_count,
            'success_rate': f"{success_count}/{len(test_queries)}",
            'pipeline_type': 'bigquery_ml_final',
            'embedding_model': self.embedding_model,
            'location': self.location,
            'results': results
        }
        
        # ê²°ê³¼ ì €ì¥
        output_file = 'bigquery_ml_rag_results_final.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… BigQuery ML RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ì„±ê³µ: {success_count}/{len(test_queries)} ì¿¼ë¦¬")
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ëŠ” ì„¤ì •
    project_id = "persona-diary-service"
    dataset_id = "nebula_con_kaggle"
    location = "us-central1"
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag_pipeline = RAGPipelineBigQueryMLFinal(project_id, dataset_id, location)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "How to optimize machine learning models?",
        "Best practices for data science projects?",
        "Startup advice for new founders",
        "PhD vs startup career path",
        "Machine learning in production"
    ]
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = rag_pipeline.run_full_pipeline_test(test_queries)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ BigQuery ML RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
    print(f"âœ… embedding_model_test ëª¨ë¸ ì‚¬ìš©!")
    print(f"âœ… BigQuery MLê³¼ Vertex AI ì™„ë²½ ì—°ë™!")
    print(f"ğŸš€ ì´ì œ ìºê¸€ í•´ì»¤í†¤ ì œì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")


if __name__ == "__main__":
    main() 