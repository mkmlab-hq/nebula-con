#!/usr/bin/env python3
"""
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸
BigQuery MLê³¼ Vertex AIë¥¼ í™œìš©í•œ HackerNews ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
"""

import json
import logging
import os
from typing import Any, Dict

from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, project_id: str, dataset_id: str):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.client = bigquery.Client()
        self.model_path = (
            f"`{project_id}.{dataset_id}.text_embedding_remote_model`"
        )

        logger.info("âœ… RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project_id}, ë°ì´í„°ì…‹: {dataset_id}")

    def create_embeddings_table(self, source_table: str,
                               target_table: str) -> bool:
        """ì„ë² ë”© í…Œì´ë¸” ìƒì„±"""
        try:
            # BigQuery ML ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ ì¿¼ë¦¬
            query = f"""
            CREATE OR REPLACE TABLE 
            `{self.project_id}.{self.dataset_id}.{target_table}` AS
            SELECT 
                id,
                title,
                text,
                ML.GENERATE_EMBEDDING(
                    MODEL {self.model_path},
                    STRUCT(
                        CONCAT(
                            IFNULL(title, ''), ' ',
                            IFNULL(text, '')
                        ) AS content
                    )
                ) AS embedding
            FROM `{self.project_id}.{self.dataset_id}.{source_table}`
            LIMIT 100
            """

            logger.info("ğŸ” ì„ë² ë”© í…Œì´ë¸” ìƒì„± ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
            query_job = self.client.query(query)
            query_job.result()

            logger.info(f"âœ… ì„ë² ë”© í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {target_table}")
            return True

        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def retrieve_and_generate(self, query_text: str,
                             embeddings_table: str,
                             top_k: int = 5) -> Dict[str, Any]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        try:
            # 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            embedding_query = f"""
            SELECT 
                ML.GENERATE_EMBEDDING(
                    MODEL {self.model_path},
                    STRUCT('{query_text}' AS content)
                ) AS query_embedding
            LIMIT 1
            """

            logger.info("ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘...")
            query_job = self.client.query(embedding_query)
            query_result = list(query_job.result())

            if not query_result:
                raise ValueError("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")

            query_embedding = query_result[0].query_embedding

            # 2. ìœ ì‚¬ë„ ê²€ìƒ‰ (ML.DISTANCE ì‚¬ìš©)
            search_query = f"""
            SELECT 
                id,
                title,
                text,
                ML.DISTANCE(embedding, '{query_embedding}') AS distance
            FROM `{self.project_id}.{self.dataset_id}.{embeddings_table}`
            ORDER BY distance ASC
            LIMIT {top_k}
            """

            logger.info("ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            search_job = self.client.query(search_query)
            search_results = list(search_job.result())

            # 3. AI ë‹µë³€ ìƒì„±
            context = "\n".join([
                f"ì œëª©: {row.title}\në‚´ìš©: {row.text}"
                for row in search_results
            ])

            ai_query = f"""
            SELECT 
                AI.GENERATE_TEXT(
                    'ë‹¤ìŒ HackerNews ê²Œì‹œê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. '
                    'ì§ˆë¬¸: {query_text}\n\n'
                    'ì°¸ê³  ìë£Œ:\n{context}',
                    'gemini-pro'
                ) AS answer
            LIMIT 1
            """

            logger.info("ğŸ” AI ë‹µë³€ ìƒì„± ì¤‘...")
            ai_job = self.client.query(ai_query)
            ai_result = list(ai_job.result())

            if not ai_result:
                raise ValueError("AI ë‹µë³€ ìƒì„± ì‹¤íŒ¨")

            answer = ai_result[0].answer

            return {
                "query": query_text,
                "context": context,
                "answer": answer,
                "sources": [
                    {
                        "id": row.id,
                        "title": row.title,
                        "distance": row.distance
                    }
                    for row in search_results
                ]
            }

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    def run_full_pipeline(self, source_table: str = "hackernews_data",
                         embeddings_table: str = "hackernews_embeddings") -> bool:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰ ì‹œì‘...")

            # 1. ì„ë² ë”© í…Œì´ë¸” ìƒì„±
            if not self.create_embeddings_table(source_table, embeddings_table):
                return False

            # 2. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_queries = [
                "What are the latest trends in AI?",
                "How to optimize machine learning models?",
                "Best practices for data science projects?"
            ]

            results = []
            for query in test_queries:
                logger.info(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰: {query}")
                result = self.retrieve_and_generate(query, embeddings_table)
                results.append(result)

                if "error" in result:
                    logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {result['error']}")
                    return False

            # 3. ê²°ê³¼ ì €ì¥
            output_file = "rag_pipeline_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            logger.info("âœ… RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")

            return True

        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project_id = os.environ.get('GOOGLE_CLOUD_PROJECT',
                                   'persona-diary-service')
        dataset_id = os.environ.get('BIGQUERY_DATASET', 'nebula_con_kaggle')

        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ì‹¤í–‰
        pipeline = RAGPipeline(project_id, dataset_id)
        success = pipeline.run_full_pipeline()

        if success:
            print("ğŸ‰ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
            print("ê²°ê³¼ íŒŒì¼: rag_pipeline_results.json")
        else:
            print("âŒ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1

        return 0

    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main())
