#!/usr/bin/env python3
"""
ğŸ† ìºê¸€ BigQuery AI í•´ì»¤í†¤ - ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1

ëª©í‘œ: Week 1 ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1 êµ¬ì¶•
ê¸°ê°„: 2025.8.15-8.17 (48ì‹œê°„)
ìš°ì„ ìˆœìœ„: ğŸš¨ ê¸´ê¸‰

êµ¬í˜„ ë‚´ìš©:
- ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
- Chunking ì•Œê³ ë¦¬ì¦˜
- ë²¡í„°í™” ë° ì €ì¥ ì‹œìŠ¤í…œ
- ê¸°ë³¸ ê²€ìƒ‰ ëª¨ë¸
- ê°„ë‹¨í•œ RAG ì²´ì¸
"""

import json
import logging
import os
import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class DataPipelineV1:
    """Week 1 ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1 êµ¬í˜„"""

    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self.config = {
            "chunk_size": 512,
            "chunk_overlap": 0.1,
            "vector_dimension": 768,
            "max_memory_gb": 4,
            "target_accuracy": 0.6,
            "max_response_time": 2.0,
        }

        # ë°ì´í„° ì €ì¥ì†Œ
        self.chunks = []
        self.vectors = None
        self.metadata = []

        logger.info(f"ğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1 ì´ˆê¸°í™” ì™„ë£Œ: {datetime.now()}")

    def load_sample_data(self) -> pd.DataFrame:
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë”© (í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Stack Overflow ìŠ¤íƒ€ì¼)
            sample_data = [
                {
                    "id": 1,
                    "title": "How to implement machine learning pipeline?",
                    "body": "I am working on a machine learning project and need help with implementing a data pipeline. I have data in CSV format and want to create a system that can process, clean, and analyze the data automatically. What are the best practices for building such a pipeline?",
                    "tags": "machine-learning,python,scikit-learn",
                    "score": 15,
                },
                {
                    "id": 2,
                    "title": "BigQuery performance optimization tips",
                    "body": "What are the best practices for optimizing BigQuery performance? I have a large dataset with millions of rows and need to improve query performance. I am using standard SQL and want to know about partitioning, clustering, and other optimization techniques.",
                    "tags": "bigquery,google-cloud,performance",
                    "score": 23,
                },
                {
                    "id": 3,
                    "title": "Natural language processing with transformers",
                    "body": "I want to build an NLP system using transformer models. Can someone help me understand the architecture and implementation details? I am particularly interested in BERT and GPT models for text classification and generation tasks.",
                    "tags": "nlp,transformers,pytorch",
                    "score": 31,
                },
                {
                    "id": 4,
                    "title": "Data preprocessing best practices",
                    "body": "What are the essential data preprocessing steps for machine learning projects? I want to understand data cleaning, normalization, feature engineering, and handling missing values. Any practical examples would be helpful.",
                    "tags": "data-preprocessing,machine-learning,python",
                    "score": 18,
                },
                {
                    "id": 5,
                    "title": "Model evaluation and validation strategies",
                    "body": "How do you evaluate and validate machine learning models effectively? I need to understand cross-validation, metrics selection, and avoiding overfitting. What are the common pitfalls and best practices?",
                    "tags": "model-evaluation,validation,machine-learning",
                    "score": 25,
                },
            ]

            df = pd.DataFrame(sample_data)

            # CSVë¡œ ì €ì¥
            csv_path = self.data_dir / "sample_questions.csv"
            df.to_csv(csv_path, index=False)

            logger.info(f"âœ… ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)}ê°œ, ì €ì¥: {csv_path}")
            return df

        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            if pd.isna(text):
                return ""

            # ê¸°ë³¸ ì •ê·œí™”
            text = str(text).lower().strip()

            # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
            text = text.replace("\n", " ").replace("\r", " ")

            # ì—°ì† ê³µë°± ì œê±°
            text = " ".join(text.split())

            return text

        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""

    def create_chunks(
        self, text: str, chunk_size: int = None, overlap: float = None
    ) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            chunk_size = chunk_size or self.config["chunk_size"]
            overlap = overlap or self.config["chunk_overlap"]

            if not text:
                return []

            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
            words = text.split()

            if len(words) <= chunk_size:
                return [text]

            chunks = []
            step = int(chunk_size * (1 - overlap))

            for i in range(0, len(words), step):
                chunk_words = words[i : i + chunk_size]
                chunk_text = " ".join(chunk_words)
                if chunk_text.strip():
                    chunks.append(chunk_text)

            logger.info(
                f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ (í¬ê¸°: {chunk_size}, ì¤‘ë³µ: {overlap})"
            )
            return chunks

        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def vectorize_text(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (TF-IDF ê¸°ë°˜)"""
        try:
            # ê°„ë‹¨í•œ TF-IDF êµ¬í˜„
            words = text.lower().split()

            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            word_freq = {}
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1

            # ë²¡í„° ìƒì„± (ê³ ì • ì°¨ì›)
            vector = np.zeros(self.config["vector_dimension"])

            # í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„±
            for word, freq in word_freq.items():
                hash_val = hash(word) % self.config["vector_dimension"]
                vector[hash_val] = freq

            # ì •ê·œí™”
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm

            return vector

        except Exception as e:
            logger.error(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return np.zeros(self.config["vector_dimension"])

    def process_data(self, df: pd.DataFrame) -> bool:
        """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        try:
            logger.info("ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ê²°í•©
            df["combined_text"] = df["title"].fillna("") + " " + df["body"].fillna("")

            # ì „ì²˜ë¦¬
            df["processed_text"] = df["combined_text"].apply(self.preprocess_text)

            # ì²­í‚¹
            all_chunks = []
            chunk_metadata = []

            for idx, row in df.iterrows():
                chunks = self.create_chunks(row["processed_text"])

                for chunk_idx, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    chunk_metadata.append(
                        {
                            "doc_id": row["id"],
                            "chunk_id": f"{row['id']}_{chunk_idx}",
                            "title": row["title"],
                            "tags": row["tags"],
                            "score": row["score"],
                            "chunk_index": chunk_idx,
                            "total_chunks": len(chunks),
                        }
                    )

            self.chunks = all_chunks
            self.metadata = chunk_metadata

            # ë²¡í„°í™”
            logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹œì‘...")
            vectors = []
            for chunk in self.chunks:
                vector = self.vectorize_text(chunk)
                vectors.append(vector)

            self.vectors = np.array(vectors)

            # ê²°ê³¼ ì €ì¥
            self.save_results()

            logger.info(
                f"âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬, {self.vectors.shape} ë²¡í„°"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    def save_results(self):
        """ì²˜ë¦¬ ê²°ê³¼ ì €ì¥"""
        try:
            # ì²­í¬ ì €ì¥
            chunks_path = self.data_dir / "processed_chunks.json"
            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = self.data_dir / "chunk_metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)

            # ë²¡í„° ì €ì¥ (NumPy í˜•ì‹)
            vectors_path = self.data_dir / "vectors.npy"
            np.save(vectors_path, self.vectors)

            # ì„¤ì • ì €ì¥
            config_path = self.data_dir / "pipeline_config.json"
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)

            logger.info(
                f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {chunks_path}, {metadata_path}, {vectors_path}"
            )

        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)"""
        try:
            if self.vectors is None or len(self.chunks) == 0:
                logger.error("âŒ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.vectorize_text(query)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, vector in enumerate(self.vectors):
                similarity = np.dot(query_vector, vector)
                similarities.append((i, similarity))

            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x[1], reverse=True)

            # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            results = []
            for i, (chunk_idx, similarity) in enumerate(similarities[:top_k]):
                results.append(
                    {
                        "rank": i + 1,
                        "chunk_id": self.metadata[chunk_idx]["chunk_id"],
                        "title": self.metadata[chunk_idx]["title"],
                        "tags": self.metadata[chunk_idx]["tags"],
                        "score": self.metadata[chunk_idx]["score"],
                        "chunk_text": self.chunks[chunk_idx][:200] + "...",
                        "similarity": float(similarity),
                    }
                )

            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_rag_response(self, query: str, top_k: int = 3) -> str:
        """RAG ë‹µë³€ ìƒì„± (ê¸°ë³¸ í”„ë¡¬í”„íŠ¸)"""
        try:
            # ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            search_results = self.search(query, top_k)

            if not search_results:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join(
                [
                    f"ì œëª©: {result['title']}\në‚´ìš©: {result['chunk_text']}\níƒœê·¸: {result['tags']}"
                    for result in search_results
                ]
            )

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {query}

ì°¸ê³  ì •ë³´:
{context}

ë‹µë³€:"""

            # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©)
            response = f"ì§ˆë¬¸ '{query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\n"
            response += "ì°¸ê³ í•œ ì •ë³´:\n"
            for result in search_results:
                response += (
                    f"- {result['title']} (ìœ ì‚¬ë„: {result['similarity']:.3f})\n"
                )

            response += f"\nì´ {len(search_results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."

            logger.info(f"âœ… RAG ë‹µë³€ ìƒì„± ì™„ë£Œ: '{query}'")
            return response

        except Exception as e:
            logger.error(f"âŒ RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def evaluate_performance(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í‰ê°€"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_usage = (
                self.vectors.nbytes / (1024**3) if self.vectors is not None else 0
            )

            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • (ì‹œë®¬ë ˆì´ì…˜)
            processing_time = len(self.chunks) * 0.01  # ì²­í¬ë‹¹ 0.01ì´ˆ ê°€ì •

            # ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            test_queries = [
                "machine learning",
                "bigquery optimization",
                "natural language processing",
            ]

            search_times = []
            for query in test_queries:
                start_time = datetime.now()
                self.search(query)
                end_time = datetime.now()
                search_times.append((end_time - start_time).total_seconds())

            avg_search_time = np.mean(search_times)

            # ì„±ëŠ¥ ì§€í‘œ
            performance = {
                "total_chunks": len(self.chunks),
                "vector_dimension": self.config["vector_dimension"],
                "memory_usage_gb": round(memory_usage, 3),
                "processing_time_seconds": round(processing_time, 3),
                "avg_search_time_seconds": round(avg_search_time, 3),
                "target_memory_gb": self.config["max_memory_gb"],
                "target_search_time": self.config["max_response_time"],
                "memory_target_met": bool(memory_usage <= self.config["max_memory_gb"]),
                "search_time_target_met": bool(
                    avg_search_time <= self.config["max_response_time"]
                ),
            }

            # ì„±ëŠ¥ ì €ì¥
            perf_path = self.data_dir / "performance_metrics.json"
            with open(perf_path, "w", encoding="utf-8") as f:
                json.dump(performance, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: {perf_path}")
            return performance

        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ìºê¸€ í•´ì»¤í†¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1 ì‹¤í–‰ ì‹œì‘")

        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = DataPipelineV1()

        # 1ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„° ë¡œë”©
        logger.info("ğŸ“¥ 1ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„° ë¡œë”©")
        df = pipeline.load_sample_data()

        if df.empty:
            logger.error("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return False

        # 2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬
        logger.info("ğŸ”„ 2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬")
        if not pipeline.process_data(df):
            logger.error("âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            return False

        # 3ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ” 3ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        test_query = "machine learning pipeline"
        search_results = pipeline.search(test_query)

        if search_results:
            logger.info(f"âœ… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(search_results)}ê°œ ê²°ê³¼")
        else:
            logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

        # 4ë‹¨ê³„: RAG í…ŒìŠ¤íŠ¸
        logger.info("ğŸ¤– 4ë‹¨ê³„: RAG í…ŒìŠ¤íŠ¸")
        rag_response = pipeline.generate_rag_response(test_query)
        logger.info(f"âœ… RAG í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(rag_response)}ì ë‹µë³€")

        # 5ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€
        logger.info("ğŸ“Š 5ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€")
        performance = pipeline.evaluate_performance()

        logger.info("ğŸ‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ v1 ì‹¤í–‰ ì™„ë£Œ!")
        logger.info(
            f"ğŸ“Š ì„±ëŠ¥ ì§€í‘œ: {json.dumps(performance, indent=2, ensure_ascii=False)}"
        )

        return True

    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
