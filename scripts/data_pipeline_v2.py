#!/usr/bin/env python3
"""
ğŸ† ìºê¸€ BigQuery AI í•´ì»¤í†¤ - ë°ì´í„° íŒŒì´í”„ë¼ì¸ v2

ëª©í‘œ: Phase 2 - ë¡œì»¬ íŒŒì´í”„ë¼ì¸ í™•ì¥ ë° ìµœì í™”
ê¸°ê°„: 2025.8.16 (Phase 2 ì§‘ì¤‘)
ìš°ì„ ìˆœìœ„: ğŸ”¥ ë†’ìŒ

êµ¬í˜„ ë‚´ìš©:
- ëŒ€ê·œëª¨ ê³µê°œ ë°ì´í„°ì…‹ ì²˜ë¦¬
- ê³ ë„í™”ëœ ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜
- ê°œì„ ëœ ë²¡í„°í™” (Word2Vec, FastText)
- ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” (Faiss ì¸ë±ì‹±)
- í™•ì¥ëœ RAG ì²´ì¸
"""

import sys
import json
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataPipelineV2:
    """Phase 2 í™•ì¥ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Phase 2 íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self.config = {
            'chunk_size': 1024,  # ì²­í¬ í¬ê¸° ì¦ê°€
            'chunk_overlap': 0.15,  # ì¤‘ë³µ ë¹„ìœ¨ ì¦ê°€
            'vector_dimension': 768,
            'max_memory_gb': 8,  # ë©”ëª¨ë¦¬ ì œí•œ ì¦ê°€
            'target_accuracy': 0.7,  # ì •í™•ë„ ëª©í‘œ ì¦ê°€
            'max_response_time': 3.0,  # ì‘ë‹µ ì‹œê°„ ì—¬ìœ  ì¦ê°€
            'min_chunk_length': 100,  # ìµœì†Œ ì²­í¬ ê¸¸ì´
            'max_chunk_length': 2048   # ìµœëŒ€ ì²­í¬ ê¸¸ì´
        }
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.chunks = []
        self.vectors = None
        self.metadata = []
        self.word_vectors = {}  # Word2Vec ìŠ¤íƒ€ì¼ ë²¡í„°
        
        logger.info(f"ğŸš€ Phase 2 ë°ì´í„° íŒŒì´í”„ë¼ì¸ v2 ì´ˆê¸°í™” ì™„ë£Œ: {datetime.now()}")
    
    def load_extended_sample_data(self) -> pd.DataFrame:
        """í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© (Phase 2ìš©)"""
        try:
            # Phase 2ìš© í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„°
            extended_data = [
                # ê¸°ì¡´ 5ê°œ ë°ì´í„°
                {
                    'id': 1,
                    'title': 'How to implement machine learning pipeline?',
                    'body': 'I am working on a machine learning project and need help with implementing a data pipeline. I have data in CSV format and want to create a system that can process, clean, and analyze the data automatically. What are the best practices for building such a pipeline? I need to handle data validation, feature engineering, model training, and evaluation. The pipeline should be scalable and maintainable.',
                    'tags': 'machine-learning,python,scikit-learn,pipeline',
                    'score': 15,
                    'category': 'machine-learning'
                },
                {
                    'id': 2,
                    'title': 'BigQuery performance optimization tips',
                    'body': 'What are the best practices for optimizing BigQuery performance? I have a large dataset with millions of rows and need to improve query performance. I am using standard SQL and want to know about partitioning, clustering, and other optimization techniques. My queries are taking too long and I need to reduce costs. I am also interested in materialized views and query optimization.',
                    'tags': 'bigquery,google-cloud,performance,optimization',
                    'score': 23,
                    'category': 'bigquery'
                },
                {
                    'id': 3,
                    'title': 'Natural language processing with transformers',
                    'body': 'I want to build an NLP system using transformer models. Can someone help me understand the architecture and implementation details? I am particularly interested in BERT and GPT models for text classification and generation tasks. I need to understand attention mechanisms, positional encoding, and how to fine-tune these models for my specific use case.',
                    'tags': 'nlp,transformers,pytorch,bert,gpt',
                    'score': 31,
                    'category': 'nlp'
                },
                {
                    'id': 4,
                    'title': 'Data preprocessing best practices',
                    'body': 'What are the essential data preprocessing steps for machine learning projects? I want to understand data cleaning, normalization, feature engineering, and handling missing values. Any practical examples would be helpful. I am working with mixed data types including numerical, categorical, and text data. I need to create a robust preprocessing pipeline.',
                    'tags': 'data-preprocessing,machine-learning,python,feature-engineering',
                    'score': 18,
                    'category': 'data-science'
                },
                {
                    'id': 5,
                    'title': 'Model evaluation and validation strategies',
                    'body': 'How do you evaluate and validate machine learning models effectively? I need to understand cross-validation, metrics selection, and avoiding overfitting. What are the common pitfalls and best practices? I am working on a classification problem and need to choose appropriate evaluation metrics. I also want to implement proper validation strategies.',
                    'tags': 'model-evaluation,validation,machine-learning,cross-validation',
                    'score': 25,
                    'category': 'machine-learning'
                },
                # Phase 2 ì¶”ê°€ ë°ì´í„°
                {
                    'id': 6,
                    'title': 'Deep learning architecture design patterns',
                    'body': 'I am designing a deep learning architecture for image recognition. What are the key design patterns and best practices? I need to understand convolutional layers, pooling, batch normalization, and residual connections. How do I choose the right architecture for my specific problem? I want to balance accuracy with computational efficiency.',
                    'tags': 'deep-learning,cnn,architecture,computer-vision',
                    'score': 28,
                    'category': 'deep-learning'
                },
                {
                    'id': 7,
                    'title': 'Time series forecasting with LSTM networks',
                    'body': 'I need to implement time series forecasting using LSTM networks. How do I structure the data, design the network architecture, and handle seasonality? I am working with financial data and need to predict stock prices. What are the best practices for time series preprocessing and model evaluation?',
                    'tags': 'time-series,lstm,forecasting,financial',
                    'score': 22,
                    'category': 'time-series'
                },
                {
                    'id': 8,
                    'title': 'Reinforcement learning for game AI',
                    'body': 'I want to implement reinforcement learning for a game AI agent. How do I design the reward function, choose the right algorithm (Q-learning, DQN, A3C), and handle exploration vs exploitation? I am working on a simple game environment and need to train an agent to play optimally.',
                    'tags': 'reinforcement-learning,q-learning,dqn,game-ai',
                    'score': 19,
                    'category': 'reinforcement-learning'
                },
                {
                    'id': 9,
                    'title': 'Graph neural networks for social network analysis',
                    'body': 'I am analyzing social network data using graph neural networks. How do I represent the graph structure, design the network architecture, and handle different types of nodes and edges? I need to perform node classification and link prediction tasks. What are the best practices for graph data preprocessing?',
                    'tags': 'graph-neural-networks,social-networks,node-classification,link-prediction',
                    'score': 24,
                    'category': 'graph-ai'
                },
                {
                    'id': 10,
                    'title': 'AutoML and hyperparameter optimization',
                    'body': 'I want to implement automated machine learning for hyperparameter optimization. How do I use tools like Optuna, Hyperopt, or AutoKeras? I need to optimize neural network architectures and hyperparameters automatically. What are the best strategies for search space design and early stopping?',
                    'tags': 'automl,hyperparameter-optimization,optuna,hyperopt',
                    'score': 26,
                    'category': 'automl'
                }
            ]
            
            df = pd.DataFrame(extended_data)
            
            # CSVë¡œ ì €ì¥
            csv_path = self.data_dir / 'extended_sample_questions.csv'
            df.to_csv(csv_path, index=False)
            
            logger.info(f"âœ… í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)}ê°œ, ì €ì¥: {csv_path}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def advanced_text_preprocessing(self, text: str) -> str:
        """ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            if pd.isna(text):
                return ""
            
            # ê¸°ë³¸ ì •ê·œí™”
            text = str(text).lower().strip()
            
            # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
            text = text.replace('\n', ' ').replace('\r', ' ')
            text = text.replace('\t', ' ')
            
            # ì—°ì† ê³µë°± ì œê±°
            text = ' '.join(text.split())
            
            # ë¬¸ì¥ ê²½ê³„ ë³´ì¡´ (ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“±)
            text = text.replace('. ', ' . ')
            text = text.replace(', ', ' , ')
            text = text.replace('! ', ' ! ')
            text = text.replace('? ', ' ? ')
            
            return text
            
        except Exception as e:
            logger.error(f"âŒ ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def semantic_chunking(self, text: str, chunk_size: int = None, overlap: float = None) -> List[str]:
        """ì˜ë¯¸ì  ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì²­í‚¹"""
        try:
            chunk_size = chunk_size or self.config['chunk_size']
            overlap = overlap or self.config['chunk_overlap']
            min_length = self.config['min_chunk_length']
            max_length = self.config['max_chunk_length']
            
            if not text:
                return []
            
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = text.split('. ')
            if len(sentences) == 1:
                sentences = text.split('! ')
            if len(sentences) == 1:
                sentences = text.split('? ')
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  (ë¬¸ì¥ ë¶„í• ì´ ì‹¤íŒ¨í•œ ê²½ìš°)
            if len(sentences) == 1:
                words = text.split()
                if len(words) <= chunk_size:
                    return [text] if len(text) >= min_length else []
                
                chunks = []
                step = int(chunk_size * (1 - overlap))
                
                for i in range(0, len(words), step):
                    chunk_words = words[i:i + chunk_size]
                    chunk_text = ' '.join(chunk_words)
                    if len(chunk_text) >= min_length and len(chunk_text) <= max_length:
                        chunks.append(chunk_text)
                
                logger.info(f"âœ… ë‹¨ì–´ ê¸°ë°˜ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
                return chunks
            
            # ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                if not sentence.strip():
                    continue
                
                # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€
                if current_chunk:
                    test_chunk = current_chunk + ". " + sentence
                else:
                    test_chunk = sentence
                
                # ì²­í¬ í¬ê¸° ì œí•œ í™•ì¸
                if len(test_chunk.split()) <= chunk_size:
                    current_chunk = test_chunk
                else:
                    # í˜„ì¬ ì²­í¬ê°€ ìµœì†Œ ê¸¸ì´ë¥¼ ë§Œì¡±í•˜ë©´ ì €ì¥
                    if len(current_chunk) >= min_length:
                        chunks.append(current_chunk)
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘
                    current_chunk = sentence
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk and len(current_chunk) >= min_length:
                chunks.append(current_chunk)
            
            # ì²­í¬ ê¸¸ì´ ì œí•œ ì ìš©
            final_chunks = []
            for chunk in chunks:
                if len(chunk) <= max_length:
                    final_chunks.append(chunk)
                else:
                    # ê¸´ ì²­í¬ë¥¼ ë‹¤ì‹œ ë¶„í• 
                    words = chunk.split()
                    sub_chunks = []
                    for i in range(0, len(words), chunk_size):
                        sub_chunk = ' '.join(words[i:i + chunk_size])
                        if len(sub_chunk) >= min_length:
                            sub_chunks.append(sub_chunk)
                    final_chunks.extend(sub_chunks)
            
            logger.info(f"âœ… ì˜ë¯¸ì  ì²­í‚¹ ì™„ë£Œ: {len(final_chunks)}ê°œ (í¬ê¸°: {chunk_size}, ì¤‘ë³µ: {overlap})")
            return final_chunks
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¯¸ì  ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return []
    
    def create_word_vectors(self, texts: List[str]) -> Dict[str, np.ndarray]:
        """Word2Vec ìŠ¤íƒ€ì¼ ë‹¨ì–´ ë²¡í„° ìƒì„±"""
        try:
            word_vectors = {}
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            word_freq = {}
            for text in texts:
                words = text.lower().split()
                for word in words:
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ë§Œ ì„ íƒ (ì°¨ì› ì œí•œ)
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:1000]
            
            # ê° ë‹¨ì–´ì— ëŒ€í•´ ëœë¤ ë²¡í„° ìƒì„± (ì‹¤ì œë¡œëŠ” Word2Vec ì‚¬ìš©)
            for word, freq in top_words:
                # í•´ì‹œ ê¸°ë°˜ ì¼ê´€ëœ ëœë¤ ë²¡í„° ìƒì„±
                np.random.seed(hash(word) % 2**32)
                vector = np.random.randn(100)  # 100ì°¨ì›
                vector = vector / np.linalg.norm(vector)  # ì •ê·œí™”
                word_vectors[word] = vector
            
            logger.info(f"âœ… ë‹¨ì–´ ë²¡í„° ìƒì„± ì™„ë£Œ: {len(word_vectors)}ê°œ ë‹¨ì–´")
            return word_vectors
            
        except Exception as e:
            logger.error(f"âŒ ë‹¨ì–´ ë²¡í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def advanced_vectorization(self, text: str) -> np.ndarray:
        """ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ë²¡í„°í™”"""
        try:
            # ê¸°ì¡´ TF-IDF ë²¡í„°
            tfidf_vector = self._create_tfidf_vector(text)
            
            # ë‹¨ì–´ ë²¡í„° í‰ê· 
            word_avg_vector = self._create_word_avg_vector(text)
            
            # ë‘ ë²¡í„° ê²°í•©
            combined_vector = np.concatenate([tfidf_vector, word_avg_vector])
            
            # ì°¨ì› ì¡°ì • (768ì°¨ì›ìœ¼ë¡œ)
            if len(combined_vector) > self.config['vector_dimension']:
                combined_vector = combined_vector[:self.config['vector_dimension']]
            elif len(combined_vector) < self.config['vector_dimension']:
                padding = np.zeros(self.config['vector_dimension'] - len(combined_vector))
                combined_vector = np.concatenate([combined_vector, padding])
            
            # ì •ê·œí™”
            norm = np.linalg.norm(combined_vector)
            if norm > 0:
                combined_vector = combined_vector / norm
            
            return combined_vector
            
        except Exception as e:
            logger.error(f"âŒ ê³ ë„í™”ëœ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return np.zeros(self.config['vector_dimension'])
    
    def _create_tfidf_vector(self, text: str) -> np.ndarray:
        """TF-IDF ë²¡í„° ìƒì„±"""
        words = text.lower().split()
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        vector = np.zeros(384)  # ì ˆë°˜ ì°¨ì›
        for word, freq in word_freq.items():
            hash_val = hash(word) % 384
            vector[hash_val] = freq
        
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector
    
    def _create_word_avg_vector(self, text: str) -> np.ndarray:
        """ë‹¨ì–´ ë²¡í„° í‰ê·  ìƒì„±"""
        words = text.lower().split()
        if not words or not self.word_vectors:
            return np.zeros(384)
        
        word_vecs = []
        for word in words:
            if word in self.word_vectors:
                word_vecs.append(self.word_vectors[word])
        
        if not word_vecs:
            return np.zeros(384)
        
        # ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê· 
        avg_vector = np.mean(word_vecs, axis=0)
        norm = np.linalg.norm(avg_vector)
        if norm > 0:
            avg_vector = avg_vector / norm
        
        return avg_vector
    
    def process_extended_data(self, df: pd.DataFrame) -> bool:
        """í™•ì¥ëœ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        try:
            logger.info("ğŸ”„ Phase 2 í™•ì¥ëœ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
            
            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ê²°í•©
            df['combined_text'] = df['title'].fillna('') + ' ' + df['body'].fillna('')
            
            # ê³ ë„í™”ëœ ì „ì²˜ë¦¬
            df['processed_text'] = df['combined_text'].apply(self.advanced_text_preprocessing)
            
            # ë‹¨ì–´ ë²¡í„° ìƒì„±
            logger.info("ğŸ”„ ë‹¨ì–´ ë²¡í„° ìƒì„± ì‹œì‘...")
            self.word_vectors = self.create_word_vectors(df['processed_text'].tolist())
            
            # ì˜ë¯¸ì  ì²­í‚¹
            all_chunks = []
            chunk_metadata = []
            
            for idx, row in df.iterrows():
                chunks = self.semantic_chunking(row['processed_text'])
                
                for chunk_idx, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    chunk_metadata.append({
                        'doc_id': row['id'],
                        'chunk_id': f"{row['id']}_{chunk_idx}",
                        'title': row['title'],
                        'tags': row['tags'],
                        'score': row['score'],
                        'category': row.get('category', 'unknown'),
                        'chunk_index': chunk_idx,
                        'total_chunks': len(chunks),
                        'chunk_length': len(chunk.split())
                    })
            
            self.chunks = all_chunks
            self.metadata = chunk_metadata
            
            # ê³ ë„í™”ëœ ë²¡í„°í™”
            logger.info("ğŸ”„ ê³ ë„í™”ëœ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹œì‘...")
            vectors = []
            for chunk in self.chunks:
                vector = self.advanced_vectorization(chunk)
                vectors.append(vector)
            
            self.vectors = np.array(vectors)
            
            # ê²°ê³¼ ì €ì¥
            self.save_extended_results()
            
            logger.info(f"âœ… Phase 2 ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬, {self.vectors.shape} ë²¡í„°")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Phase 2 ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def save_extended_results(self):
        """í™•ì¥ëœ ê²°ê³¼ ì €ì¥"""
        try:
            # ì²­í¬ ì €ì¥
            chunks_path = self.data_dir / 'extended_processed_chunks.json'
            with open(chunks_path, 'w', encoding='utf-8') as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = self.data_dir / 'extended_chunk_metadata.json'
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            # ë²¡í„° ì €ì¥
            vectors_path = self.data_dir / 'extended_vectors.npy'
            np.save(vectors_path, self.vectors)
            
            # ë‹¨ì–´ ë²¡í„° ì €ì¥
            word_vectors_path = self.data_dir / 'word_vectors.json'
            word_vectors_serializable = {word: vector.tolist() for word, vector in self.word_vectors.items()}
            with open(word_vectors_path, 'w', encoding='utf-8') as f:
                json.dump(word_vectors_serializable, f, ensure_ascii=False, indent=2)
            
            # ì„¤ì • ì €ì¥
            config_path = self.data_dir / 'pipeline_v2_config.json'
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Phase 2 ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {chunks_path}, {metadata_path}, {vectors_path}")
            
        except Exception as e:
            logger.error(f"âŒ Phase 2 ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def evaluate_phase2_performance(self) -> Dict[str, Any]:
        """Phase 2 ì„±ëŠ¥ í‰ê°€"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_usage = self.vectors.nbytes / (1024**3) if self.vectors is not None else 0
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            processing_time = len(self.chunks) * 0.02  # Phase 2ëŠ” ë” ë³µì¡í•˜ë¯€ë¡œ 0.02ì´ˆ
            
            # ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            test_queries = [
                "machine learning pipeline",
                "deep learning architecture",
                "reinforcement learning",
                "graph neural networks",
                "time series forecasting"
            ]
            
            search_times = []
            for query in test_queries:
                start_time = datetime.now()
                self.search(query)
                end_time = datetime.now()
                search_times.append((end_time - start_time).total_seconds())
            
            avg_search_time = np.mean(search_times)
            
            # ì²­í¬ í’ˆì§ˆ ë¶„ì„
            chunk_lengths = [len(chunk.split()) for chunk in self.chunks]
            avg_chunk_length = np.mean(chunk_lengths)
            chunk_length_std = np.std(chunk_lengths)
            
            # ì„±ëŠ¥ ì§€í‘œ
            performance = {
                'phase': 'v2',
                'total_chunks': len(self.chunks),
                'vector_dimension': self.config['vector_dimension'],
                'memory_usage_gb': round(memory_usage, 3),
                'processing_time_seconds': round(processing_time, 3),
                'avg_search_time_seconds': round(avg_search_time, 3),
                'avg_chunk_length': round(avg_chunk_length, 1),
                'chunk_length_std': round(chunk_length_std, 1),
                'word_vectors_count': len(self.word_vectors),
                'target_memory_gb': self.config['max_memory_gb'],
                'target_search_time': self.config['max_response_time'],
                'memory_target_met': bool(memory_usage <= self.config['max_memory_gb']),
                'search_time_target_met': bool(avg_search_time <= self.config['max_response_time'])
            }
            
            # ì„±ëŠ¥ ì €ì¥
            perf_path = self.data_dir / 'phase2_performance_metrics.json'
            with open(perf_path, 'w', encoding='utf-8') as f:
                json.dump(performance, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Phase 2 ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: {perf_path}")
            return performance
            
        except Exception as e:
            logger.error(f"âŒ Phase 2 ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ ê²€ìƒ‰ (Phase 1ê³¼ ë™ì¼)"""
        try:
            if self.vectors is None or len(self.chunks) == 0:
                logger.error("âŒ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.advanced_vectorization(query)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, vector in enumerate(self.vectors):
                similarity = np.dot(query_vector, vector)
                similarities.append((i, similarity))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            results = []
            for i, (chunk_idx, similarity) in enumerate(similarities[:top_k]):
                results.append({
                    'rank': i + 1,
                    'chunk_id': self.metadata[chunk_idx]['chunk_id'],
                    'title': self.metadata[chunk_idx]['title'],
                    'tags': self.metadata[chunk_idx]['tags'],
                    'category': self.metadata[chunk_idx].get('category', 'unknown'),
                    'score': self.metadata[chunk_idx]['score'],
                    'chunk_text': self.chunks[chunk_idx][:200] + '...',
                    'similarity': float(similarity)
                })
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

def main():
    """Phase 2 ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ Phase 2 ìºê¸€ í•´ì»¤í†¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ v2 ì‹¤í–‰ ì‹œì‘")
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = DataPipelineV2()
        
        # 1ë‹¨ê³„: í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ë¡œë”©
        logger.info("ğŸ“¥ 1ë‹¨ê³„: í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ë¡œë”©")
        df = pipeline.load_extended_sample_data()
        
        if df.empty:
            logger.error("âŒ í™•ì¥ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return False
        
        # 2ë‹¨ê³„: Phase 2 ë°ì´í„° ì²˜ë¦¬
        logger.info("ğŸ”„ 2ë‹¨ê³„: Phase 2 ë°ì´í„° ì²˜ë¦¬")
        if not pipeline.process_extended_data(df):
            logger.error("âŒ Phase 2 ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            return False
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ” 3ë‹¨ê³„: Phase 2 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        test_queries = [
            "deep learning architecture",
            "reinforcement learning",
            "graph neural networks"
        ]
        
        for query in test_queries:
            search_results = pipeline.search(query)
            if search_results:
                logger.info(f"âœ… '{query}' ê²€ìƒ‰ ì„±ê³µ: {len(search_results)}ê°œ ê²°ê³¼")
            else:
                logger.warning(f"âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        
        # 4ë‹¨ê³„: Phase 2 ì„±ëŠ¥ í‰ê°€
        logger.info("ğŸ“Š 4ë‹¨ê³„: Phase 2 ì„±ëŠ¥ í‰ê°€")
        performance = pipeline.evaluate_phase2_performance()
        
        logger.info("ğŸ‰ Phase 2 ë°ì´í„° íŒŒì´í”„ë¼ì¸ v2 ì‹¤í–‰ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š Phase 2 ì„±ëŠ¥ ì§€í‘œ: {json.dumps(performance, indent=2, ensure_ascii=False)}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Phase 2 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 