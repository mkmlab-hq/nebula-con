#!/usr/bin/env python3
"""
ì‹¤ì œ ìŒì„± ë¶„ì„ ëª¨ë“ˆ
Phase 2: ì‹¤ì œ AI ë¶„ì„ ë¡œì§ êµ¬í˜„
"""

import librosa
import numpy as np
from scipy import signal
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import logging
import soundfile as sf

class RealVoiceAnalyzer:
    """ì‹¤ì œ ìŒì„± ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.sample_rate = 22050  # ê¸°ë³¸ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        
    def load_audio(self, audio_path: str) -> Optional[Tuple[np.ndarray, int]]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        try:
            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            return audio, sr
        except Exception as e:
            self.logger.error(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_mfcc_features(self, audio: np.ndarray) -> np.ndarray:
        """MFCC (Mel-frequency cepstral coefficients) ì¶”ì¶œ"""
        try:
            # MFCC ì¶”ì¶œ (13ê°œ ê³„ìˆ˜)
            mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)
            return mfcc
        except Exception as e:
            self.logger.error(f"MFCC ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros((13, 1))
    
    def extract_spectral_features(self, audio: np.ndarray) -> Dict:
        """ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ ì£¼íŒŒìˆ˜
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate)[0]
            
            # ìŠ¤í™íŠ¸ëŸ´ ëŒ€ë¹„
            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=self.sample_rate)
            
            # ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate)[0]
            
            # ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate)[0]
            
            return {
                'centroids': spectral_centroids,
                'contrast': spectral_contrast,
                'rolloff': spectral_rolloff,
                'bandwidth': spectral_bandwidth
            }
        except Exception as e:
            self.logger.error(f"ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def extract_pitch_features(self, audio: np.ndarray) -> Dict:
        """í”¼ì¹˜ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # í”¼ì¹˜ ì¶”ì¶œ
            pitches, magnitudes = librosa.piptrack(y=audio, sr=self.sample_rate)
            
            # í‰ê·  í”¼ì¹˜
            pitch_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:
                    pitch_values.append(pitch)
            
            if len(pitch_values) > 0:
                mean_pitch = np.mean(pitch_values)
                pitch_std = np.std(pitch_values)
                pitch_range = np.max(pitch_values) - np.min(pitch_values)
            else:
                mean_pitch = 0
                pitch_std = 0
                pitch_range = 0
            
            return {
                'mean_pitch': mean_pitch,
                'pitch_std': pitch_std,
                'pitch_range': pitch_range
            }
        except Exception as e:
            self.logger.error(f"í”¼ì¹˜ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'mean_pitch': 0, 'pitch_std': 0, 'pitch_range': 0}
    
    def extract_energy_features(self, audio: np.ndarray) -> Dict:
        """ì—ë„ˆì§€ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # RMS ì—ë„ˆì§€
            rms = librosa.feature.rms(y=audio)[0]
            
            # ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸
            zcr = librosa.feature.zero_crossing_rate(audio)[0]
            
            # ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼
            energy_entropy = -np.sum(rms * np.log(rms + 1e-10))
            
            return {
                'rms_mean': np.mean(rms),
                'rms_std': np.std(rms),
                'zcr_mean': np.mean(zcr),
                'energy_entropy': energy_entropy
            }
        except Exception as e:
            self.logger.error(f"ì—ë„ˆì§€ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'rms_mean': 0, 'rms_std': 0, 'zcr_mean': 0, 'energy_entropy': 0}
    
    def analyze_emotion_from_voice(self, audio: np.ndarray) -> Dict:
        """ìŒì„±ì—ì„œ ê°ì • ë¶„ì„"""
        try:
            # MFCC íŠ¹ì„±
            mfcc = self.extract_mfcc_features(audio)
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            
            # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„±
            spectral_features = self.extract_spectral_features(audio)
            
            # í”¼ì¹˜ íŠ¹ì„±
            pitch_features = self.extract_pitch_features(audio)
            
            # ì—ë„ˆì§€ íŠ¹ì„±
            energy_features = self.extract_energy_features(audio)
            
            # ê°ì • ì§€ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
            emotion_scores = {
                'happiness': 0.0,
                'sadness': 0.0,
                'anger': 0.0,
                'calmness': 0.0,
                'excitement': 0.0
            }
            
            # í”¼ì¹˜ ê¸°ë°˜ ê°ì • ë¶„ì„
            if pitch_features['mean_pitch'] > 200:  # ë†’ì€ í”¼ì¹˜
                emotion_scores['excitement'] += 0.3
                emotion_scores['happiness'] += 0.2
            elif pitch_features['mean_pitch'] < 100:  # ë‚®ì€ í”¼ì¹˜
                emotion_scores['sadness'] += 0.3
                emotion_scores['calmness'] += 0.2
            
            # ì—ë„ˆì§€ ê¸°ë°˜ ê°ì • ë¶„ì„
            if energy_features['rms_mean'] > 0.1:  # ë†’ì€ ì—ë„ˆì§€
                emotion_scores['excitement'] += 0.2
                emotion_scores['anger'] += 0.1
            elif energy_features['rms_mean'] < 0.05:  # ë‚®ì€ ì—ë„ˆì§€
                emotion_scores['sadness'] += 0.2
                emotion_scores['calmness'] += 0.3
            
            # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„± ê¸°ë°˜ ë¶„ì„
            if 'centroids' in spectral_features:
                centroid_mean = np.mean(spectral_features['centroids'])
                if centroid_mean > 2000:  # ë†’ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„
                    emotion_scores['excitement'] += 0.2
                elif centroid_mean < 1000:  # ë‚®ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„
                    emotion_scores['calmness'] += 0.2
            
            # ì •ê·œí™”
            total_score = sum(emotion_scores.values())
            if total_score > 0:
                emotion_scores = {k: v/total_score for k, v in emotion_scores.items()}
            
            # ì£¼ìš” ê°ì • ê²°ì •
            primary_emotion = max(emotion_scores, key=emotion_scores.get)
            
            return {
                'emotion_scores': emotion_scores,
                'primary_emotion': primary_emotion,
                'confidence': emotion_scores[primary_emotion]
            }
            
        except Exception as e:
            self.logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'emotion_scores': {'calmness': 1.0},
                'primary_emotion': 'calmness',
                'confidence': 0.5
            }
    
    def analyze_voice_characteristics(self, audio_path: str) -> Dict:
        """ìŒì„± íŠ¹ì„± ì¢…í•© ë¶„ì„"""
        try:
            self.logger.info("ì‹¤ì œ ìŒì„± ë¶„ì„ ì‹œì‘")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio_data = self.load_audio(audio_path)
            if audio_data is None:
                raise ValueError("ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨")
            
            audio, sr = audio_data
            
            # ê°ì • ë¶„ì„
            emotion_result = self.analyze_emotion_from_voice(audio)
            
            # ìŒì„± íŠ¹ì„± ë¶„ì„
            pitch_features = self.extract_pitch_features(audio)
            energy_features = self.extract_energy_features(audio)
            
            # ìŒì„± í’ˆì§ˆ í‰ê°€
            snr = self.calculate_snr(audio)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "emotion": emotion_result['primary_emotion'],
                "emotion_confidence": round(emotion_result['confidence'], 2),
                "pitch_mean": round(pitch_features['mean_pitch'], 1),
                "energy_level": self.categorize_energy(energy_features['rms_mean']),
                "voice_quality": self.categorize_quality(snr),
                "speaking_rate": self.calculate_speaking_rate(audio),
                "analysis_quality": "ì‹¤ì œ ìŒì„± ë¶„ì„",
                "confidence": 0.80
            }
            
            self.logger.info(f"ìŒì„± ë¶„ì„ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"ìŒì„± íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "emotion": "calmness",
                "emotion_confidence": 0.5,
                "pitch_mean": 150.0,
                "energy_level": "ë³´í†µ",
                "voice_quality": "ë³´í†µ",
                "speaking_rate": "ë³´í†µ",
                "analysis_quality": "ê¸°ë³¸ê°’ (ë¶„ì„ ì‹¤íŒ¨)",
                "confidence": 0.0
            }
    
    def calculate_snr(self, audio: np.ndarray) -> float:
        """ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ SNR ì¶”ì •
            signal_power = np.mean(audio**2)
            noise_floor = np.percentile(audio**2, 10)
            snr = 10 * np.log10(signal_power / (noise_floor + 1e-10))
            return snr
        except:
            return 20.0  # ê¸°ë³¸ê°’
    
    def categorize_energy(self, rms_mean: float) -> str:
        """ì—ë„ˆì§€ ìˆ˜ì¤€ ë¶„ë¥˜"""
        if rms_mean > 0.1:
            return "ë†’ìŒ"
        elif rms_mean > 0.05:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def categorize_quality(self, snr: float) -> str:
        """ìŒì„± í’ˆì§ˆ ë¶„ë¥˜"""
        if snr > 30:
            return "ìš°ìˆ˜"
        elif snr > 20:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def calculate_speaking_rate(self, audio: np.ndarray) -> str:
        """ë§í•˜ê¸° ì†ë„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ë§í•˜ê¸° ì†ë„ ì¶”ì •
            zcr = librosa.feature.zero_crossing_rate(audio)[0]
            avg_zcr = np.mean(zcr)
            
            if avg_zcr > 0.1:
                return "ë¹ ë¦„"
            elif avg_zcr > 0.05:
                return "ë³´í†µ"
            else:
                return "ëŠë¦¼"
        except:
            return "ë³´í†µ"

def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    analyzer = RealVoiceAnalyzer()
    
    # í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    test_audio = "test_data/test_voice.wav"
    
    print("ğŸ¤ ì‹¤ì œ ìŒì„± ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    result = analyzer.analyze_voice_characteristics(test_audio)
    
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼:")
    for key, value in result.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    main() 