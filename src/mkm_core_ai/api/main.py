# master_data_collector_v3_web.py
import requests
import xml.etree.ElementTree as ET
import feedparser
import pandas as pd
import time
from tqdm import tqdm
import logging
import sys
from collections import deque
import random
from fastapi import FastAPI, BackgroundTasks, HTTPException
import uvicorn

# --- ì‘ì „ ì„¤ì • ---
RUN_DURATION_HOURS = 12  # ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ì´ ì‹œê°„
RUN_DURATION_SECONDS = RUN_DURATION_HOURS * 3600

# --- FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
app = FastAPI(
    title="í”„ë¡œë©”í…Œìš°ìŠ¤ ìë£Œ ìˆ˜ì§‘ê¸° API",
    description="Cloud Schedulerì™€ ì—°ë™í•˜ì—¬ ë§ˆë¼í†¤ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•˜ëŠ” API",
    version="3.0"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ê¸°ì¡´ ìˆ˜ì§‘ê¸° í•¨ìˆ˜ë“¤ (ìˆ˜ì • ì—†ìŒ) ---
def search_pubmed(query, max_results=2000, collected_ids=set()):
    """PubMedì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , ì¤‘ë³µì„ í”¼í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    logging.info(f"PubMedì—ì„œ '{query}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    # ... (ì´í•˜ ë¡œì§ì€ v2ì™€ ë™ì¼) ...
    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
    search_params = {"db": "pubmed", "term": query, "retmax": max_results, "usehistory": "y"}
    try:
        search_response = requests.get(base_url + "esearch.fcgi", params=search_params, timeout=30)
        search_response.raise_for_status()
        search_root = ET.fromstring(search_response.content)
        id_list = [id_elem.text for id_elem in search_root.findall(".//Id")]
        if not id_list: return [], set()
        new_ids = [pmid for pmid in id_list if pmid not in collected_ids]
        if not new_ids: return [], set()
        web_env = search_root.find(".//WebEnv").text
        query_key = search_root.find(".//QueryKey").text
        fetch_params = {"db": "pubmed", "retmode": "xml", "webenv": web_env, "query_key": query_key, "id": ",".join(new_ids)}
        fetch_response = requests.get(base_url + "efetch.fcgi", params=fetch_params, timeout=60)
        fetch_response.raise_for_status()
        articles_root = ET.fromstring(fetch_response.content)
        articles, newly_collected_ids = [], set()
        for article_elem in articles_root.findall(".//PubmedArticle"):
            try:
                pmid = article_elem.find(".//PMID").text
                title = article_elem.find(".//ArticleTitle").text
                abstract_elem = article_elem.find(".//AbstractText")
                abstract = abstract_elem.text if abstract_elem is not None else ""
                authors_list = [f"{author.find('LastName').text} {author.find('Initials').text}" for author in article_elem.findall(".//Author") if author.find('LastName') is not None and author.find('Initials') is not None]
                articles.append({"id": pmid, "title": title, "authors": ", ".join(authors_list), "abstract": abstract, "source": "PubMed", "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"})
                newly_collected_ids.add(pmid)
            except Exception as e: logging.error(f"PubMed ì•„í‹°í´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        logging.info(f"PubMedì—ì„œ '{query}' í‚¤ì›Œë“œë¡œ {len(articles)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return articles, newly_collected_ids
    except requests.exceptions.RequestException as e:
        logging.error(f"PubMed API ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
        return [], set()

def search_arxiv(query, max_results=2000, collected_ids=set()):
    """arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ , ì¤‘ë³µì„ í”¼í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    logging.info(f"arXivì—ì„œ '{query}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    # ... (ì´í•˜ ë¡œì§ì€ v2ì™€ ë™ì¼) ...
    base_url = "http://export.arxiv.org/api/query?"
    params = {"search_query": f'all:"{query}"', "start": 0, "max_results": max_results, "sortBy": "relevance"}
    try:
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        articles, newly_collected_ids = [], set()
        for entry in feed.entries:
            try:
                entry_id = entry.id.split('/abs/')[-1]
                if entry_id in collected_ids: continue
                articles.append({"id": entry_id, "title": entry.title.replace('\n', ' ').strip(), "authors": ", ".join(author.name for author in entry.authors), "abstract": entry.summary.replace('\n', ' ').strip(), "source": "arXiv", "url": entry.link})
                newly_collected_ids.add(entry_id)
            except Exception as e: logging.error(f"arXiv ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        logging.info(f"arXivì—ì„œ '{query}' í‚¤ì›Œë“œë¡œ {len(articles)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return articles, newly_collected_ids
    except requests.exceptions.RequestException as e:
        logging.error(f"arXiv API ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
        return [], set()

def expand_keywords(articles, current_keywords_queue, processed_keywords):
    """ìˆ˜ì§‘ëœ ë…¼ë¬¸ ì´ˆë¡ì—ì„œ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ íì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    # ... (ì´í•˜ ë¡œì§ì€ v2ì™€ ë™ì¼) ...
    new_keywords_found = 0
    for article in random.sample(articles, min(len(articles), 10)):
        words = article.get('abstract', '').lower().split()
        for i in range(len(words) - 1):
            potential_keyword = f"{words[i]} {words[i+1]}"
            if potential_keyword not in processed_keywords and len(potential_keyword) > 8:
                current_keywords_queue.append(potential_keyword)
                processed_keywords.add(potential_keyword)
                new_keywords_found += 1
                if new_keywords_found >= 5: return
    if new_keywords_found > 0: logging.info(f"{new_keywords_found}ê°œì˜ ìƒˆë¡œìš´ íƒìƒ‰ í‚¤ì›Œë“œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")


# --- ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  ì‹¤ì œ ì‘ì—… í•¨ìˆ˜ ---
def run_marathon_collection_task():
    """12ì‹œê°„ ë™ì•ˆ ë°ì´í„° ìˆ˜ì§‘ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë¡œì§"""
    logging.info("ë°±ê·¸ë¼ìš´ë“œ ë§ˆë¼í†¤ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    start_time = time.time()
    
    SEED_KEYWORDS = [
        "voice health analysis", "biomedical signal processing", "photoplethysmography",
        "rPPG", "traditional medicine diagnosis AI", "tongue diagnosis", "facial diagnosis",
        "medical image analysis", "wearable sensor data", "health monitoring system",
        "deep learning in healthcare", "acoustic analysis of voice", "jitter", "shimmer",
        "harmonics-to-noise ratio", "cardiovascular signal", "pulse wave analysis"
    ]
    
    keywords_queue = deque(SEED_KEYWORDS)
    processed_keywords = set(SEED_KEYWORDS)
    collected_pubmed_ids = set()
    collected_arxiv_ids = set()
    all_articles_df = pd.DataFrame()
    
    while time.time() - start_time < RUN_DURATION_SECONDS:
        if not keywords_queue:
            logging.warning("íƒìƒ‰í•  í‚¤ì›Œë“œê°€ ëª¨ë‘ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œë“œ í‚¤ì›Œë“œë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
            keywords_queue.extend(SEED_KEYWORDS)

        keyword = keywords_queue.popleft()
        
        # PubMed, arXiv ì²˜ë¦¬ ë¡œì§ì€ v2ì™€ ë™ì¼
        pubmed_articles, new_pubmed_ids = search_pubmed(keyword, collected_ids=collected_pubmed_ids)
        if pubmed_articles:
            collected_pubmed_ids.update(new_pubmed_ids)
            temp_df = pd.DataFrame(pubmed_articles)
            all_articles_df = pd.concat([all_articles_df, temp_df], ignore_index=True)
            expand_keywords(pubmed_articles, keywords_queue, processed_keywords)

        arxiv_articles, new_arxiv_ids = search_arxiv(keyword, collected_ids=collected_arxiv_ids)
        if arxiv_articles:
            collected_arxiv_ids.update(new_arxiv_ids)
            temp_df = pd.DataFrame(arxiv_articles)
            all_articles_df = pd.concat([all_articles_df, temp_df], ignore_index=True)
            expand_keywords(arxiv_articles, keywords_queue, processed_keywords)
        
        # ì£¼ê¸°ì  ì €ì¥ (íŒŒì¼ ì´ë¦„ì— ë‚ ì§œ/ì‹œê°„ ì¶”ê°€)
        if int(time.time() - start_time) % 300 == 0: # 5ë¶„ë§ˆë‹¤ ì €ì¥
            filename = f"marathon_collection_live_{time.strftime('%Y%m%d_%H%M%S')}.csv"
            all_articles_df.drop_duplicates(subset=['id', 'source'], inplace=True)
            all_articles_df.to_csv(filename, index=False, encoding="utf-8-sig")
            logging.info(f"ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {filename} ({len(all_articles_df)}í¸)")

        time.sleep(2)

    logging.info(f"ì´ {RUN_DURATION_HOURS}ì‹œê°„ì˜ ë§ˆë¼í†¤ ìˆ˜ì§‘ ì‘ì „ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    final_filename = f"marathon_collection_final_{time.strftime('%Y%m%d_%H%M%S')}.csv"
    all_articles_df.drop_duplicates(subset=['id', 'source'], inplace=True)
    all_articles_df.to_csv(final_filename, index=False, encoding="utf-8-sig")
    logging.info(f"ìµœì¢…ì ìœ¼ë¡œ ì´ {len(all_articles_df)}í¸ì˜ ë…¼ë¬¸ ì •ë³´ë¥¼ '{final_filename}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# --- ğŸ”¥ í•µì‹¬ ìˆ˜ì •: API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.post("/collect-data")
async def trigger_collection(background_tasks: BackgroundTasks):
    """
    Cloud Schedulerì˜ í˜¸ì¶œì„ ë°›ì•„ ë°ì´í„° ìˆ˜ì§‘ì„ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
    """
    logging.info("'/collect-data' ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    # 12ì‹œê°„ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì¶”ê°€
    background_tasks.add_task(run_marathon_collection_task)
    # ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŒì„ ì¦‰ì‹œ í´ë¼ì´ì–¸íŠ¸(Cloud Scheduler)ì— ì•Œë¦¼
    return {"status": "success", "message": "12-hour data collection task has been started in the background."}

@app.get("/")
def read_root():
    return {"message": "MKM Data Collector API is running."}

# --- ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    # ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•˜ë©´, uvicorn ì„œë²„ê°€ ë¡œì»¬ì—ì„œ ê°€ë™ë©ë‹ˆë‹¤.
    # Cloud Run ë°°í¬ ì‹œì—ëŠ” ì´ ë¶€ë¶„ì´ ì•„ë‹Œ, gunicornê³¼ ê°™ì€ WSGI ì„œë²„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    print("ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. http://127.0.0.1:8000")
    uvicorn.run(app, host="127.0.0.1", port=8000)
